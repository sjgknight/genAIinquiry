[{"id":1,"order":"1.00000000000000000000","Risk-challenge":"AI provides opportunities for students to generate content worthy of submission as their own work.","Source":[{"id":64,"value":"64"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":2,"order":"2.00000000000000000000","Risk-challenge":"One of the chief concerns expressed about the use of generative AI in higher education relates to the potential for students to misuse the technology, for example to generate entire assignments or components thereof, not referencing appropriately, relying on AI rather than generating their own ideas, or using it to complete online exams.","Source":[{"id":63,"value":"63"}],"Tags":[{"id":977308,"value":"integrity","color":"darker-gray"}],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":3,"order":"3.00000000000000000000","Risk-challenge":"One of the key elements of research integrity that the Go8 believes needs to be addressed explicitly in the context of generative AI is that of maintaining data confidentiality. Research studies â€“ particularly in health and medical disciplines â€“ will often include patient confidential information that can only be used under strict conditions. In some cases, there are also additional and special restrictions around Australian Government medical data related to Medicare and Pharmaceutical Benefits Scheme (PBS) databases. Any use of generative AI tools such as ChatGPT for the analysis of such data runs the risk of releasing the data in an uncontrolled and unauthorised manner.","Source":[{"id":63,"value":"63"}],"Tags":[{"id":977308,"value":"integrity","color":"darker-gray"}],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":4,"order":"4.00000000000000000000","Risk-challenge":"Diverse teams need to be encouraged to create, test and adapt these tools for their own\nneeds and values. Many of the risks and challenges of generative AI tools go back to\nwho has created them, as well as the underlying social and environmental impacts of\ntool development and use. Generative AI, like all other models, will have inbuilt\nstructures and biases, based on the choices made in development of its building blocks\n(e.g. in data sets, sensors, models, networks, infrastructures and how they are\nconstructed and curated over time).","Source":[{"id":62,"value":"62"}],"Tags":[],"Recommendations":[{"id":56,"value":"need to support and enable through proactive programs, First Nations inclusion and leadership"}],"Recommendations - Address-challenge":[]},{"id":5,"order":"5.00000000000000000000","Risk-challenge":"It will be critical that students learn about and not just with Generative AI. As our\neducational programs have shown, education in an environment of rapidly changing technology must critically engage with that technology as a system to give a basic understanding of how it was made, where it is used, and where it might be going. Giving students the opportunity to learn about this context, and to provide opportunities for them to tinker with AI and imagine themselves being able to shape it in future jobs, is critical not just to better education, but also to making these systems safer, more sustainable and more responsible. We should focus more on learning about Generative AI, so that children and people of all ages have the opportunity to understand it, even if their access to it is limited.","Source":[{"id":62,"value":"62"}],"Tags":[],"Recommendations":[{"id":57,"value":"The inquiry should encompass how to introduce content relating to understanding Generative AI into the national curriculum."}],"Recommendations - Address-challenge":[]},{"id":6,"order":"6.00000000000000000000","Risk-challenge":"Data Privacy and Security,","Source":[{"id":1,"value":"1"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":7,"order":"7.00000000000000000000","Risk-challenge":"Algorithmic bias and fairness,","Source":[{"id":1,"value":"1"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":8,"order":"8.00000000000000000000","Risk-challenge":"Lack of Human Interaction and Social Skills Development,","Source":[{"id":1,"value":"1"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":9,"order":"9.00000000000000000000","Risk-challenge":"Dependency on Technology","Source":[{"id":1,"value":"1"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":10,"order":"10.00000000000000000000","Risk-challenge":"Access control and age restriction,","Source":[{"id":2,"value":"2"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":11,"order":"11.00000000000000000000","Risk-challenge":"content hallucination-bias-and-moderation,","Source":[{"id":2,"value":"2"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":12,"order":"12.00000000000000000000","Risk-challenge":"data privacy and security,","Source":[{"id":2,"value":"2"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":13,"order":"13.00000000000000000000","Risk-challenge":"integrity and plagiarism,","Source":[{"id":2,"value":"2"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":14,"order":"14.00000000000000000000","Risk-challenge":"broad impacts on issues such as wellbeing","Source":[{"id":2,"value":"2"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":15,"order":"15.00000000000000000000","Risk-challenge":"Overuse, underuse and misuse, including inappropriate shortcuts,","Source":[{"id":3,"value":"3"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":16,"order":"16.00000000000000000000","Risk-challenge":"automation bias (failing to check outputs)","Source":[{"id":3,"value":"3"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":17,"order":"17.00000000000000000000","Risk-challenge":"Limits to creativity,","Source":[{"id":5,"value":"5"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":18,"order":"17.00000000000000000000","Risk-challenge":"loss of personal voice,","Source":[{"id":5,"value":"5"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":19,"order":"18.00000000000000000000","Risk-challenge":"commercial interests and IP concerns,","Source":[{"id":5,"value":"5"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":20,"order":"19.00000000000000000000","Risk-challenge":"equity issues in access and benefits from tools,","Source":[{"id":5,"value":"5"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":21,"order":"20.00000000000000000000","Risk-challenge":"Assurance of learning, data bias,","Source":[{"id":6,"value":"6"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":22,"order":"21.00000000000000000000","Risk-challenge":"adequate understanding of the materials being generated (particularly for PhD research contexts),","Source":[{"id":6,"value":"6"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":23,"order":"22.00000000000000000000","Risk-challenge":"regulatory lag,","Source":[{"id":6,"value":"6"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":24,"order":"23.00000000000000000000","Risk-challenge":"accessibility for all students,","Source":[{"id":6,"value":"6"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":25,"order":"24.00000000000000000000","Risk-challenge":"training adequacy,","Source":[{"id":6,"value":"6"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":26,"order":"25.00000000000000000000","Risk-challenge":"intellectual property,","Source":[{"id":6,"value":"6"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":27,"order":"26.00000000000000000000","Risk-challenge":"AI accuracy and hallucinations","Source":[{"id":6,"value":"6"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":28,"order":"27.00000000000000000000","Risk-challenge":"decrease critical thinking,","Source":[{"id":7,"value":"7"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":29,"order":"28.00000000000000000000","Risk-challenge":"plagiarism","Source":[{"id":7,"value":"7"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":30,"order":"29.00000000000000000000","Risk-challenge":"Assurance of learning,","Source":[{"id":8,"value":"8"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":31,"order":"30.00000000000000000000","Risk-challenge":"bias,","Source":[{"id":8,"value":"8"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":32,"order":"31.00000000000000000000","Risk-challenge":"privacy","Source":[{"id":8,"value":"8"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":33,"order":"32.00000000000000000000","Risk-challenge":"â€¢  However, Victorian English teachers noted that currently there is insufficient support and direction available in curriculum documentation for them to confidently engage with and use Generative AI in the English classroom.","Source":[{"id":10,"value":"10"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":34,"order":"33.00000000000000000000","Risk-challenge":"â€¢  In addition, teachers noted that there are few policies in place in their teaching context, nor direction or guidance from key leadership bodies around the use of Generative AI, for them to confidently engage with the use of these tools in the English classroom. This raises concerns about the ethical and just use of these tools.","Source":[{"id":10,"value":"10"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":35,"order":"34.00000000000000000000","Risk-challenge":"â€¢  English teachers are concerned about the way Generative AI can be consciously or unconsciously misused by students, adversely impacting their literary competence and development and the joyful experience of learning.","Source":[{"id":10,"value":"10"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":36,"order":"35.00000000000000000000","Risk-challenge":"â€¢  English teachers see their role in the classroom as integral to the development of civic-minded, critical-thinking, confident, and compassionate young people; Generative AI in its current form is viewed as a tool not able to replicate the role of the classroom teacher.","Source":[{"id":10,"value":"10"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":37,"order":"36.00000000000000000000","Risk-challenge":"plagiarism and copyright infringement;","Source":[{"id":12,"value":"12"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":38,"order":"37.00000000000000000000","Risk-challenge":"unethical conduct from educators and students;","Source":[{"id":12,"value":"12"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":39,"order":"38.00000000000000000000","Risk-challenge":"bias and discrimination;","Source":[{"id":12,"value":"12"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":40,"order":"39.00000000000000000000","Risk-challenge":"concerns about academic integrity generally;","Source":[{"id":12,"value":"12"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":41,"order":"40.00000000000000000000","Risk-challenge":"privacy of data accessed by AI;","Source":[{"id":12,"value":"12"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":42,"order":"41.00000000000000000000","Risk-challenge":"lack of transparency/ explainabiity;","Source":[{"id":12,"value":"12"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":43,"order":"42.00000000000000000000","Risk-challenge":"ongoing monitoring and evaluation","Source":[{"id":12,"value":"12"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":44,"order":"43.00000000000000000000","Risk-challenge":"AI Literacy; Ethics; Changes to assessment practices: Less emphasis on the need for knowledge to be remembered/retained at a certain point (exams) but more emphasis on its construction and use (projects)\nâ— Requirement for learning journals, prompt histories, screenshots of interactions with AI as part of assignment or essay submissions\nâ— Rethinking what â€˜open bookâ€™ exams looks like - could/should they include AI?\nâ— Shifting some assessment tasks to become more authentic tasks by assessing a studentâ€™s application of skills as opposed to the content they produce.","Source":[{"id":23,"value":"23"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":45,"order":"44.00000000000000000000","Risk-challenge":"Lack of funding for AI research in Australia compared with other countries","Source":[{"id":24,"value":"24"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":46,"order":"45.00000000000000000000","Risk-challenge":"The primacy of relationships in education outcomes must not be undermined by AI technologies.\nIn consultation with teachers, careful review of current curriculums should be undertaken to ensure the sequential and age-appropriate development of ethics and critical literacy skills in students.\nTeachers need timely and ongoing access to high-quality professional development related to AI in education.\nCommercial interests must be subservient to the over-riding interest of teacher and student wellbeing.\nThe professional judgement of the teacher is fundamental to the learning and assessment process. It must be respected and protected.","Source":[{"id":26,"value":"26"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":47,"order":"46.00000000000000000000","Risk-challenge":"Educators will need to review what is assessed, whether knowledge or the process of learning is assessed, and revise learning outcomes. This will require further review of the ways in which knowledge is assessed in different disciplines. A broader implication is that the higher education sector will need to collectively rethink the nature and purposes of assessment.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":48,"order":"47.00000000000000000000","Risk-challenge":"Expectations of acceptable use and forms of knowledge production vary across disciplines. Staff in teaching and assessment roles may also have different expectations.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":49,"order":"48.00000000000000000000","Risk-challenge":"Levels of understanding, knowledge and skills related to the use of generative AI vary among staff and students and across cohorts. Threshold standards of AI literacy are recommended for higher education staff and students.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":50,"order":"49.00000000000000000000","Risk-challenge":"Generative AI may promote a shift towards oral assessments or other forms of assessment with administrative and practical resourcing implications. These forms of assessment may also be less appropriate to meet diverse learning needs.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":51,"order":"50.00000000000000000000","Risk-challenge":"Generative AI may have a negative impact on the development of writing and academic skills, knowledge acquisition and application and critical thinking. The widespread use of AI tools could influence the development of social and interpersonal skills and lead to dependence on generative AI to solve complex problems without considerations of the potential limitations of or risks associated with generated content. Generative AI also has potential implications for student civic development and democratic engagement, given the process of generating material through generative AI without broader personal or professional engagement, and the potential of AI generated material to reinforce biases and stereotypes.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":52,"order":"51.00000000000000000000","Risk-challenge":"Institutional guidelines are needed to protect the privacy of student data and their intellectual property.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":53,"order":"52.00000000000000000000","Risk-challenge":"Generative AI has potential to contribute to work insecurity and reductions in the education workforce.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":54,"order":"53.00000000000000000000","Risk-challenge":"Provision of clear and consistent standards for using generative AI, guidelines for acceptable and unacceptable use and development of AI literacy are key challenges for higher education providers.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":55,"order":"54.00000000000000000000","Risk-challenge":"The need for institutional academic integrity policies and documentation to provide clear guidance to students and staff in the appropriate and inappropriate use of generative AI is increasingly evident. The AAIN publication released in March 2023 provides initial guidelines. The AAIN publication released in May 2023 summarises institutional responses to the use of","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":56,"order":"55.00000000000000000000","Risk-challenge":"generative AI and provides examples of good practice.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":57,"order":"56.00000000000000000000","Risk-challenge":"Strategies and resourcing are needed to address significant risks of misuse and falsification by students claiming the outputs of generative AI as their original work. The capacity of the sector to identify inappropriate use of generative AI is currently limited, particularly given the emergent nature and widespread accessibility of generative AI tools and large size of many higher education classes. While a number of companies have software that is designed to detect work generated by AI, these tools are still in early stages of development. Where they are used, they may not provide educators and institutions with robust evidence of breaches of academic integrity. Many universities have made the decision not to use detection tools until they are more mature.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":58,"order":"57.00000000000000000000","Risk-challenge":"Inappropriate use of generative AI in assessment tasks may pose risks to the reputations of individual students, institutions, and the sector. Studentsâ€™ professional careers may be at risk.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":59,"order":"58.00000000000000000000","Risk-challenge":"Risks of factual inaccuracies and biases in AI-generated work including the tendency for","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":60,"order":"59.00000000000000000000","Risk-challenge":"generative AI to produce plausible but incorrect responses, and its inability to join discrete concepts in ways that appear to be logical may have an impact on student learning or conceptions of knowledge in different fields of study. Outputs from generative AI tools may reinforce bias, prejudice, and misinformation as â€˜truthâ€™. Detecting inaccuracies may be difficult due to systems, processes, and limitations of existing software.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":61,"order":"60.00000000000000000000","Risk-challenge":"Biases within generative AI models may produce factual inaccuracies about Indigenous peoples, cultural practices and equivalent issues affecting other cultural groups.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":62,"order":"61.00000000000000000000","Risk-challenge":"Any development of generative AI standards in Australia should include reference to Indigenous Cultural and Intellectual Property (ICIP) as well as Indigenous Data Sovereignty (IDS). Risks that Indigenous knowledges and intellectual property will be incorporated into generative AI and used without appropriate attribution or acknowledgement should be minimised. The sector standard for ICIP is Terri Janke's True Tracks and for IDS, Maiam Nayri Wingara","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":63,"order":"62.00000000000000000000","Risk-challenge":"Standards for the safe and secure use of generative AI tools in relation to intellectual property, and the potential for data and privacy breaches including online data tracking have not yet been established. Likewise consistent standards for the storage of personal data and interactions with generative AI have not yet been established.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":64,"order":"63.00000000000000000000","Risk-challenge":"Generative AI tools create potential risks for transnational exposure of data which may have future implications for data regulation.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":65,"order":"64.00000000000000000000","Risk-challenge":"The use of generative AI to produce artifacts that appear to meet learning outcomes or provide evidence of knowledge and skills has potential to undermine the reputation of higher education. A lack of public confidence in the development of knowledge and skills and assurance of learning could undermine the value of qualifications.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":66,"order":"65.00000000000000000000","Risk-challenge":"Many staff and students lack a detailed understanding of how generative AI models operate. This includes understanding the centrality of â€˜temperatureâ€™ in generative AI models. Temperature determines the plausibility of responses produced by generative AI from the knowledge it has stored from different devices and information sources, and how this knowledge frames the plausibility of its outputs on a scale between factual information and creativity. Each generative AI tool has a particular â€˜temperatureâ€™ between 0 and 1, which for Chat GPT is reported to be 0.7, indicating a level of â€˜creativityâ€™ in its outputs.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":67,"order":"66.00000000000000000000","Risk-challenge":"The use of generative AI in institutional systems and processes may create redundancies, and a divided workforce in relation to new skills and literacies.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":68,"order":"67.00000000000000000000","Risk-challenge":"Higher education providers may not yet have established clear privacy and permissions guidelines relating to the use of generative AI tools in higher education contexts, for example in relation to the submission of student information or work into generative AI tools. Standards and/or guidelines are needed for the use of student work and sensitive information.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":69,"order":"68.00000000000000000000","Risk-challenge":"Guidelines and processes to support academic and research integrity may need to be more closely integrated in response to generative AI, especially in terms of what constitutes appropriate and ethical practice in sourcing and acknowledging information.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":70,"order":"69.00000000000000000000","Risk-challenge":"Access to generative AI tools depends on studentsâ€™ financial circumstances which may limit access to hardware and software requirements, education and training programs. Although free access to some generative AI tools may have some impact on democratisation, students from lower-income backgrounds may still face challenges in accessing these technologies. This may lead to a two-tier system, potentially increasing inequities among students. Inclusive approaches to teaching and support and equitable access for all students is key. The HEPPP (Higher Education Participation and Partnerships Program) provides a potential framework to address the disadvantage in relation to digital and AI literacy and digital access As generative AI tools mature and set subscription or purchase costs, higher education providers may need to purchase institutional licences to provide access to students and staff, in the same way that access is currently provided to generalist software, such as the Microsoft Office suite, and specialist software such as statistical analysis, modelling software, CAD software, and software for creative arts such as music production. The potential costs and scale of regular use of generative AI tools in the future is unknown and a consideration for providers and the sector more broadly.","Source":[{"id":58,"value":"58"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":71,"order":"70.00000000000000000000","Risk-challenge":"(1) external factors beyond institutional control, such as the current (or lack thereof) regulation of foundation models (and their commercial providers), which arguably shapes the broader digital supply chain, including that of generative AI and its safe use and management6. The usual level of rigour and prescription in technology and risk control is presently not fully understood nor the usual level of transparency and explanation required for institutional interrogation. This may result in potential downstream liability for an institution where there is no standardisation about release controls of those foundation models;","Source":[{"id":56,"value":"56"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":72,"order":"71.00000000000000000000","Risk-challenge":"(2) current absence of universally accepted standards or metrics for risk assessment. There are many present unknowns that do not comfortably sit with the usual rigorous structures of education, for example, the potential scale of impact, and the vulnerability profile of its â€˜consumerâ€™ (e.g., children, teenagers, and young adults). For example, there is no research consensus globally about the true impact of certain forms of AI on protected categories such as children or teenagers, areas such as â€œAI-enhanced nudgingâ€™ and â€œalgorithmic influenceâ€ on children and teenagers â€“ nudging them into certain behaviours without their awareness7;","Source":[{"id":56,"value":"56"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":73,"order":"72.00000000000000000000","Risk-challenge":"(3) limited funding and resourcing in a highly competitive market where the required niche level of skills are in high demand (e.g., evaluation expertise in responsible AI), and costed accordingly. The work and research into AI has a long gestation period, and is labour intensive (e.g., data labelling and model building);","Source":[{"id":56,"value":"56"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":74,"order":"73.00000000000000000000","Risk-challenge":"(4) the intersection of AI technologies and the breadth of activities that higher education institutions and schools are responsible for, is significant. For example, their administrative use, training in AI, and research in AI. The breadth of these activities requiring the establishment of different (but interwoven) governance frameworks to manage AI safety risks; and","Source":[{"id":56,"value":"56"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":75,"order":"74.00000000000000000000","Risk-challenge":"(5) a lack of coordination to map at a sector level functions, governance, and objectives to generate models of AI good governance.","Source":[{"id":56,"value":"56"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":76,"order":"75.00000000000000000000","Risk-challenge":"Learning practices There is substantial pedagogical value to be found in the significant risk taking involved in learning without generative AI: learners often feel uncertain about a learning situation and it is precisely by overcoming this challenge that they engage and learn. The reliance on generative AI has the potential to reduce opportunities for critical thinking and problemsolving, creativity; it may promote laziness and lack of independent thought; and it may limit the development of deep knowledge. Students may lack the confidence to engage in that first step and take the essential risk on not knowing in order to further understand a topic. In addition, AI responses can be deceptively convincing, leading to an assumption that the response generated is accurate. This therefore required a deliberative approach to learning design to ensure that independent thought, critical thinking and creativity are central to the learning experience","Source":[{"id":57,"value":"57"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":77,"order":"76.00000000000000000000","Risk-challenge":"Assessment design Changes as a reaction to academic integrity considerations will most probably result in the need for substantial assessment and task redesign. Education systems and individual institutions and processes will need to budget for the time it will take practitioners to do this work. Some of the models of assessment commonly used (for example multiple choice questions) have been adopted because educators need to be able to implement the assessment, generate marks, and provide feedback at scale and in a reasonable timeframe. If generative AI could be used to achieve different types of marking at scale then this provides us with an opportunity to consider other approaches to assessment that may better represent student learning. A risk and challenge in relation to this is that education systems including universities can be slow to change. The policies developed need to be flexible enough to adapt as the technology changes.","Source":[{"id":57,"value":"57"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":78,"order":"77.00000000000000000000","Risk-challenge":"Academic Integrity The immediate reaction by education institutions and sectors has been in relation to academic integrity. This is appropriate, however it should be noted that any tools that claim to be able to automatically detect the use of generative AI in the creation of assessment submissions (e.g. text, images, videos) will need to be continually updated as the tools and accuracy of the underlying model(s) continue to improve. In addition to tools, education policies need to be developed in relation to the acceptable use of generative AI in assessment. Some regulations can be a good thing â€“ even if itâ€™s only to help people understand how to be safe â€“ because people are not always sure what they are and are not allowed to do. Yet, these regulations will need to be dynamic to some extent and be continuously adapted to changing contexts. The use of AI tools can present a risk to academic integrity, similar to contract cheating and other more traditional forms of cheating that some learners may engage in. Educating students in the use of AI, in how it works and in understanding its impact in terms of their learning and integrity is the key way forward. Educators must design assessment that minimises the risk of AI tools supplanting the learner, to ensure that the learner can demonstrate mastery of the learning outcomes. The growth in detection tools is unlikely to completely ensure integrity while introducing risks to the student experience, for example, the risk of false positives that have the potential for serious detrimental impact","Source":[{"id":57,"value":"57"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":79,"order":"78.00000000000000000000","Risk-challenge":"Ethical data practices There are risks associated with the data that the large language models are using. Whether this data has been acquired ethically is still being challenged, with implications for multiple considerations including privacy, copyright, structural bias, data integrity and cultural safety. Also, using data and models owned by multinationals for education and then giving our studentsâ€™ data to them entails inherent risk. Students should be asked for their consent to share their documents with generative AI. In many journals, they do not allow reviews to be written by generative AI because they do not have the permission of the authors to do that. Ethics and integrity go hand in hand, but teaching students the ethical use of AI will require more than teaching them about academic integrity. Understanding the ethics would involve considering privacy, copyright, bias, misrepresentation and deception. Perhaps the use of AI tools is more about the ethics and understanding the risks than it is about technology.","Source":[{"id":57,"value":"57"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":80,"order":"79.00000000000000000000","Risk-challenge":"Models built on representative data Education needs to ensure that generative AI tools and models that are accessed are built using data that is representative and diverse. For the most part, large language models are built on data that is from English-speaking countries. There needs to be consideration of other languages, the impact that this has on ways of thinking and framing questions. Consideration particularly needs to be given to Indigenous languages and ways of knowing, the privacy of this data, the impact on the responses generated and the potential for assessment and teaching practices to further minimise the experiences and ways of knowing of Aboriginal and Torres Strait Islander peoples. These models are also largely built using data generated from contributions to text available on the internet. This does preference the ways of writing and the opinions of mostly white, male, middle and upper class professionals. Education needs to advocate to ensure that children, students and families experiencing disadvantage have a voice and that their experiences are included in the way that generative AI produces responses","Source":[{"id":57,"value":"57"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":81,"order":"80.00000000000000000000","Risk-challenge":"Access to generative AI tools Education needs to ensure that the gap that already exists for children, students and families experiencing disadvantage in terms of access to technology does not grow further through the growth and spread of generative AI. There are challenges in terms of access to technology, to reliable internet, and access to data. Any education that builds on the use of generative AI needs to consider support for students who cannot continue the connection with the technology when they leave the schoolhouse. This is particularly relevant considering the nation-wide bans on the use of mobile phones in schools. These bans have significant implications for students experiencing disadvantage to be able to seamlessly transition their learning with technology in and out of school","Source":[{"id":57,"value":"57"}],"Tags":[{"id":977963,"value":"equitable-access","color":"dark-pink"}],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":82,"order":"81.00000000000000000000","Risk-challenge":"Tools and platforms for the use of generative AI Education also needs to consider the implications of age restrictions on access to generative AI tools, with most primary school students being too young to consent to using the tools.","Source":[{"id":57,"value":"57"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":83,"order":"82.00000000000000000000","Risk-challenge":"Another risk that may arise by using Gen AI in education is automation of the teaching function which could occur without the re-evaluation of the education system within the changing world context. In such a case the AI instrumentalises the educator rather than complementing the educatorâ€™s human skills.","Source":[{"id":55,"value":"55"}],"Tags":[{"id":977965,"value":"de-professionalisation","color":"cyan"}],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":84,"order":"83.00000000000000000000","Risk-challenge":"AI detecEon tools do not work sufficiently well now, and there are mathemaEcal arguments that in the long term, it will not be possible to develop effecEve generaEve AI wriEng detecEon tools, as the improvement of generaEve AI will outpace the development of the detecEon tools (see a preprint by Sankar Sadasivan et al., 2023). If students are banned from using generaEve arEficial intelligence in unsupervised tasks, but some students use it, and we donâ€™t know because the detecEon tools donâ€™t work, it becomes difficult to judge what students are capable of. As I argue in my 2021 book Defending Assessment Security in a Digital World (Dawson, 2021), if restricEons are set that are not feasible, they make the integrity and validity of assessment worse, not be/er. In addiEon, the risk of false posiEves â€“ students accused of using generaEve AI who did not actually use it â€“ creates the potenEal for harm to student wellbeing and sector reputaEon. The onus should be on the vendors of these tools to provide transparent, independently validated evidence that their tools work. This has been done in the past with other assessment integrity tools, for example, I have led a research study with a vendor of authorship verificaEon soXware for the detecEon of contract cheaEng (Dawson et al., 2020). In the absence of this sort of evidence for generaEve arEficial intelligence detecEon soXware, and given the existence of evidence that the tools are â€œneither accurate nor reliableâ€ (Weber-Wulff et al., 2023) I make the following recommendaEon:","Source":[{"id":54,"value":"54"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":85,"order":"84.00000000000000000000","Risk-challenge":"In teaching: â€¢  Ensuring the safe use of Gen-AI by students and educators.â€¢  Raising levels of awareness and understanding among teachers and students about the strengths, limitations and risks of Gen-AI.â€¢  Overcoming perceptions that we can out-design or out-run Gen-AI by designing creative or authentic assessments that are â€˜AI-proofâ€™.â€¢  Ensuring staff and students understand what biases in Gen-AI training data can mean for information discovery, analysis and conclusion-drawing.â€¢  Ensuring students do not develop an over-reliance on Gen-AI, nor a fear of the technology or reluctance to learn about it and engage with it.â€¢  Preventing the inappropriate use of Gen-AI in assessments.â€¢  Building studentsâ€™ ability to separate human from AI content.â€¢  Reliance, at any level, on currently available AI detection software, which are inherently flawed.","Source":[{"id":44,"value":"44"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":86,"order":"85.00000000000000000000","Risk-challenge":"In research: â€¢  Privacy breaches of study participantsâ€™ personally identifiable information, through release of this information to Gen-AI tools, or the release of de-identified data in a way that allows for reidentification.â€¢  Privacy breaches of researchers/collaboratorsâ€™ personally identifiable information, through submission of this confidential information to Gen-AI tools.â€¢  Loss of control of Intellectual Property or Copyright, through the Gen-AI tools reproducing content without attribution.â€¢  Breaches of contractual obligations, including commercial contracts, data sharing agreements and material transfer agreements.â€¢  Non-compliance with the terms of funding agreements.â€¢  Inappropriate management of First Nations data.â€¢  Disclosure of sensitive environmental/ecological information.â€¢  Breach of research codes, Human Research Ethics Committee (HREC) approvals (e.g. Australian Code for the Conduct of Responsible Research).â€¢  Breach of research funding councilsâ€™ specific policies regarding Gen-AI.","Source":[{"id":44,"value":"44"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":87,"order":"86.00000000000000000000","Risk-challenge":"Other key risks for research integrity centre around lack of accountability and oversight when using Gen-AI to produce research outputs, specifically: â€¢  Inability to attribute authorship where Gen-AI has contributed content from unknown sources.â€¢  Inability to reproduce or replicate consistent results when using Gen-AI to provide content for research findings â€“ results will vary at each attempt making verification of research findings impossible.â€¢  Inability to establish and evaluate accuracy and validity of content generated by Gen-AI â€“probabilistic models that generate data may be biased, or produce â€˜hallucinationsâ€™ of inaccurate content, which can be hard to identify.â€¢  Inability to have contributory IP acknowledged by others if research outputs are incorporated into data that becomes available to the Gen-AI model, and is used elsewhere. No guarantee that licences on open data, code or images are applied (e.g. Creative Commons).â€¢  Accountability and trust â€“ personal accountability by individual researchers underpins all research. Use of Gen-AI can undermine this if its capabilities and the contribution that it makes, including risks, are not understood by researchers.","Source":[{"id":44,"value":"44"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":88,"order":"87.00000000000000000000","Risk-challenge":"Inquiry into the use of generative artificial intelligence in the Australian education system","Source":[{"id":44,"value":"44"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":89,"order":"88.00000000000000000000","Risk-challenge":"National security and foreign interference risks, arising from the public release through GenAI interfaces, of confidential data or research information, or from increasingly sophisticated cybersecurity attacks enabled by Gen-AI.","Source":[{"id":44,"value":"44"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":90,"order":"89.00000000000000000000","Risk-challenge":"Human rights and modern slavery law breaches, arising from use of certain Gen-AI tools developed without transparency and potentially through questionable labour practices.","Source":[{"id":44,"value":"44"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":91,"order":"90.00000000000000000000","Risk-challenge":"Researchers and research students who speak English as a second language may face increased pressure and risks, as they may be more likely to use LLMs that can improve written text, e.g. to help write a thesis or journal paper, in which case risk of IP loss is high.","Source":[{"id":44,"value":"44"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":92,"order":"91.00000000000000000000","Risk-challenge":"Some journal publishers have introduced rules restricting use of Gen-AI in submitted manuscripts. This varies between disciplines and publications, but may prevent some researchers from being able to publish in high impact journals.","Source":[{"id":44,"value":"44"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":93,"order":"92.00000000000000000000","Risk-challenge":"Detection rates of Gen-AI use are currently poor. This may have research integrity impacts in the future when new detection tools are applied to previously published research where Gen-AI may have been used to generate content. Detection tools may give erroneous results, and valid research outputs may be questioned in error.","Source":[{"id":44,"value":"44"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":94,"order":"93.00000000000000000000","Risk-challenge":"Reviewers of grants and awards using Gen-AI inappropriately in the process of evaluating proposals to circumvent human thought and insight","Source":[{"id":44,"value":"44"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":95,"order":"94.00000000000000000000","Risk-challenge":"In the ECU student survey, only about 30% of students who had used AI expressed someconfidence in using AI ethically. Over 50% of students were not at all confident, had mixed feelings,or felt unable to judge. With this in mind, the biggest risks to society are perhaps posed not by Generative AI but by its uncritical adoption.","Source":[{"id":17,"value":"17"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":96,"order":"95.00000000000000000000","Risk-challenge":"Bias and a lack of transparency.","Source":[{"id":17,"value":"17"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":97,"order":"96.00000000000000000000","Risk-challenge":"Privacy","Source":[{"id":17,"value":"17"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":98,"order":"97.00000000000000000000","Risk-challenge":"Human exploitation, including copyright and Intellectual Property infringement.","Source":[{"id":17,"value":"17"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":99,"order":"98.00000000000000000000","Risk-challenge":"Students acquire knowledge and skills through human interaction with staff and other students, in group assignments, practicums, assessment, and casual interactions, all of which cannot be replaced by AI. Those human skills will be even more critical in future careers. In health and medicine, for example, AI-enabled diagnostic tools could mean earlier diagnosis of serious and chronic disease, but treatment will still require well-trained health and medical personnel able to interpret the results and what they will mean for the patient. Similarly in law, generative AI could quickly summarise the relevant case law for a trial, but it will be the lawyer who decides how that information should be applied to the case in hand. In these situations, critical thinking â€“ the â€˜know whyâ€™ as well as the â€˜know what and howâ€™ â€“ will be even more important, with clear implications for course content and assessment.","Source":[{"id":28,"value":"28"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":100,"order":"99.00000000000000000000","Risk-challenge":"The use of GenAI tools in education poses risks and challenges, particularly regarding the safe andethical use of these technologies and the preservation of academic and research integrity. It is crucial to address these concerns to maintain a responsible and trustworthy educational environment. Research in AI in education and Learning Analytics has shown that the mere adoption of AI algorithms can introduce potential biases, perpetuating or amplifying existing inequalities or discriminatory practices. Therefore, safeguards must be implemented to ensure fairness and mitigate biases in thedata used for training these AI tools. Additionally, privacy and data security are of utmost importancewhen utilising AI systems that collect and analyse sensitive student information.  Prioritising transparency and accountability is essential to ensure the ethical use of AI systems, and avoid compromising the integrity of academic work or research. Establishing proper protocols and guidelines is necessary to govern the use, interpretation, and reporting of AI-generated outputs. Addressing these risks and challenges through robust regulations, transparent practices, and ongoing monitoring provides the necessary guardrails for implementation and maximises the benefits of generative AI tools in the education sector.","Source":[{"id":29,"value":"29"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":101,"order":"100.00000000000000000000","Risk-challenge":"the use of generative AI tools to produce work that is subsequently submitted forassessment is considered academic misconduct, unless permission to use such tools has been given by academic staffand their use has been acknowledged by the student. Similarly, generative AI can only be used in research outputswhere the material generated is acknowledged. generative AI tools such as ChatGPT are known to produce convincing but false information, a tendency that is referred to by developers as â€œhallucinatingâ€. Additionally, generative AI tools are fundamentally biased, replicating and reinforcing the biases present in the human-created materials on which they are trained.","Source":[{"id":34,"value":"34"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":102,"order":"101.00000000000000000000","Risk-challenge":"The rise of generative AIs presents several challenges for educators. In order to ensure that students are graduating with the skills and knowledge appropriate to their program of study academic staff must devise assessments that can accurately ascertain whether or not the student has attained the relevant skills and knowledge (and to what level). This is considered a minimum requirement under TEQSAâ€™s Higher Education Standards Framework.1 There is a risk that generative AI will make this more difficult when such AIs are able to produce outputs that the student is then assessed on. For example, students can already produce short form pieces of prose using current generative AI technologies. There is a risk in this environment that over time universities may reach a point where they can no longer assure the required learning has occurred in what they claim to be teaching.","Source":[{"id":52,"value":"52"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":103,"order":"102.00000000000000000000","Risk-challenge":"teaching staff will need to continuously develop new methods of assessment that assess students at a level beyond the levels of AIs. This entails a significant ongoing workload increase, as teaching staff will now be required to continuously re-tool assessments and workloads. Further, the amount of work needed to complete a degree may need to be increased, or devised differently, in an environment where students are using AIs. Put simply, the use of AIs in teaching and learning is highly likely to drive pedagogical change.","Source":[{"id":52,"value":"52"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":104,"order":"103.00000000000000000000","Risk-challenge":"Ideally, students should be assessed on their capacity to critically interrogate the outputs of AIs and the learning process itself, rather than the artefacts they produce that are currently taken to be evidence that learning has occurred. This is a significant paradigm shift for teaching institutions. This type of assessment will likely require more one on one student-staff interaction to ensure that skills and knowledge have truly been attained and work completed. This will be more demanding on staff and may mean that current expectations around how many students one staff member can teach need to be completely re-se","Source":[{"id":52,"value":"52"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":105,"order":"104.00000000000000000000","Risk-challenge":"The use of AI in research also presents risks around academic integrity. While institutions will be looking to manage instances of deliberate manipulation/falsification/misrepresentation and accidental or unintended corruption of data, there is a real risk that AI applications will be considerably ahead of current research integrity processes that would detect problems or irregularities. It may be that problematic research is not detected for some time, if at all, by which time there could be widespread ramifications.","Source":[{"id":52,"value":"52"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":106,"order":"105.00000000000000000000","Risk-challenge":"AI systems currently use limited sets of data that can be prone to biases based on data selection â€“ something that is opaque to staff and students. AIs can also replicate and reinforce existing social biases that are present inside of data sets, for example the underrepresentation of certain groups in certain academic fields. This is because inherent in the creation of AI is that any pre-existing bias or discrimination that is either unconsciously, or consciously, imbedded will set the â€˜learningâ€™ pattern for that AI","Source":[{"id":52,"value":"52"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":107,"order":"106.00000000000000000000","Risk-challenge":"THE RISK OF MISGUIDED TECHNOLOGICAL REDUNDANCIES There are a wide variety of tasks currently performed by staff in the academic context that institutions may look to AIs to perform partially or fully in the future.The NTEUâ€™s position is that AI is not an appropriate replacement for staff because it does not engage in critical thinking, produce genuine creativity or innovation. AI is also absent of the capacity to consider ethical component of its output â€“ it is simply using a pre-programmed ethical framework which may be structurally or systemically biased. Human staff are required to monitor and accept or reject AI outputs as appropriate.","Source":[{"id":52,"value":"52"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":108,"order":"107.00000000000000000000","Risk-challenge":"There are also concerns on how AI may impact on privacy and the security of personal data (including the use of student and staff personal data â€“ for example, unauthorised access of data, through to institutional managements using AI to mine confidential data or information in workplace relations situations).","Source":[{"id":52,"value":"52"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":109,"order":"108.00000000000000000000","Risk-challenge":"Another concern is around the usage of third-party AIs and the need for this to be disclosed to staff and students. Most advanced AIs are being developed for foreign, for-profit entities, and there is little transparency about what types of data they collect about users and how they use this. With higher education institutions â€“ both public and private â€“ routinely engaging in external contractors for the delivery of teaching, student support (both academic and welfare) as well as with the administration and technical tasks, there is already little (if any) transparency around these arrangements. The use of external AI driven platforms and services by third parties to the education providers adds further complexity and increases the potential risk of adverse consequences for staff and students.","Source":[{"id":52,"value":"52"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":110,"order":"109.00000000000000000000","Risk-challenge":"AI, and generative AI, have potential to disrupt many aspects of our lives. How this disruption unfolds, and to whom the benefits and costs accrue is fundamentally a social problem.","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":111,"order":"110.00000000000000000000","Risk-challenge":"It is crucial that investments in EdTech are underpinned by evidence that any tools used will support the outcomes they claim to target. Failure to do so is at best an opportunity cost, and at worst misinformation that could lead to significant harms. However, the evidence base for many EdTech tools and their uses in particular contexts is not strong. A recent review of the 100 most frequently used EdTech tools in US schools found that 26/100 met any level on a formal evidence scale, where the lowest level requires just that there is some formalised background evidence synthesis that supports a theory of change for the tool use. This aligns with other evidence indicating that principles derived from research on learning are often not adhered to in app development (Meyer et al., 2021) and that in the specific context of ebook apps targeting children, app features may distract from comprehension (Furenes et al., 2021; Kucirkova, 2023). This may lead to underuse, poor use, or simply poor procurement of technologies (ITSE, 2019). Australian values and the specific needs and contexts here should be an important consideration in the evaluation for particular uses of global products that are trained on web data that may not represent Australian values or experiences","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":112,"order":"111.00000000000000000000","Risk-challenge":"Assurance of learning is the most commonly discussed concern in current media coverage of genAIin higher education (Sullivan et al., 2023), this comprises two features: 1. Integrity â€“ i.e., assuring that learners are engaging in the activities they say they are. A number of helpful documents have been produced A number of helpful documents have been produced including from the UKâ€™s QAA (QAA, 2023) and JISC (Webb, 2023), the European Network for Academic Integrity (Foltynek et al., 2023), and the Australian Academic Integrity Network (AAIN) Generative AI Working Group, (AAIN, 2023a), alongside individual institutional responses (see, AAIN, 2023b) with UTS developing five student-centred principles to guide use of genAI (LX at UTS, 2023) 2. Validity â€“ i.e., assuring that the tasks we ask learners to engage with are good indicators of the outcomes claimed when gaining an accredited qualification. This feature of genAI is less explored, although see UTS principles below (LX at UTS, 2023)","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":113,"order":"112.00000000000000000000","Risk-challenge":"Learning processes and outcomes â€“ may be impacted by genGPT; it was this â€“ the concern that chatGPT would hamper critical thinking â€“ that underpinned New York City Department of Educationâ€™s ban on chatGPT in early 2023 (later lifted). Threshold concepts describe the key knowledge or concepts required in order to progress to a deeper level of understanding regarding a topic; once we grasp them, they change our understanding. They interact with particular methods and tools, such that sometimes new technologies mean we no longer need to learn a particular skill. For example, tools for calculating, from the abacus to the pocket calculator, through obviating understanding of manual use of log tables. It is not yet clear what impact being able to â€˜shortcutâ€™ via genAI will have, and where we should still require learning of the underlying composite skills in order to assure understanding of the higher-level skill.","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":114,"order":"112.00000000000000000000","Risk-challenge":"Normative features of learning â€“ are about values, or the kinds of learning we think are important. Technologies may shift these values (Heersmink & Knight, 2018), and particular domains â€“ including the professions â€“ may make value judgements regarding the skills and knowledge it is important to â€˜haveâ€™ manually, and those we can augment with technology. Understanding both these values, and what different communities are doing is important. Aspects of this concern are addressed through analyses of employer needs and university offerings (Kitto, 2022), however the impact of genAI, and the potential for genAI to support learners in navigating professional needs, is not yet clear.","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":115,"order":"113.00000000000000000000","Risk-challenge":"Human dignity and rights â€“ are expressions of fundamental values and rights, which may be impacted by genAI in a range of ways (Fergusson et al., 2023). Issues in this space include concerns regarding the right to privacy, creation and spread of misinformation, exacerbation of discrimination, impacts on intellectual property and related access to cultural heritage, and impacts on labour relations (Fergusson et al., 2023). It was this concern that appeared to underpin NSW and QLD Departments of Education from restricting chatGPT access in public schools, due to safeguarding concerns regarding potential access to explicit and harmful content (Cassidy, 2023). Understanding these issues in the particular contexts of learning environments is important for tackling them, with consideration of both the legal context â€“ as yet unclear â€“ and ethical concerns. The Electronic Privacy Information Centre (EPIC) (Fergusson et al., 2023) provide a recent overview (contextualised to US law) of genAI and fundamental rights. These risks derive from three issues: (1) harms in the nature of genAI models, their data, and use (e.g., environmental impacts); (2) harms in unintentional inappropriate use (e.g., lack of awareness regarding poor ability for certain tasks); (3) harms through intentional maleficent use (e.g., creation of misinformation). Across these, genAI has potential to foster a breakdown in trust between people, and biases in representation of people and cultures that may lead to Inquiry into the use of generative artificial intelligence in the Australian education system Submission 19 13 inequitable outcomes. It is important that learners and educators understand this context, including any legal obligations. Three particular areas of concern are: Privacy, Intellectual Property, and Information manipulation and representation.","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":116,"order":"114.00000000000000000000","Risk-challenge":"Challenge: Misalignment of professional needs â€“ that education institutions, professional bodies, and professional practice become misaligned in terms of how genAI is used and what is taught, in ways that hamper professional sectors or their learning.","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":117,"order":"115.00000000000000000000","Risk-challenge":"Challenge: Breakdowns in capacity â€“ that genAI may hamper human capacity, through (1) overreliance thus hampering human capacity through reduction in individual capabilities, (2) workforce restructuring that reduces human autonomy and input into decision processes, or (3) overwhelming systems through mass production of content.","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":118,"order":"116.00000000000000000000","Risk-challenge":"A set of skills, including technical and social skills, will underpin the effective use of genAI; correspondingly, there is a risk that failure to cultivate this capacity in Australia, or in specific sectors, or parts of the population, will lead to harms and inequities.","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":119,"order":"117.00000000000000000000","Risk-challenge":"Justice and inequitable experience of benefits and harms in genAI use: The potential of genAI to further exacerbate existing issues relating to marginalisation and poorer outcomes must be considered, alongside any potential benefits of genAI and ensuring that access to these benefits follows a justice principle; that the greatest benefits should flow to the most marginalised. Ample evidence exists that while existing technologies provide significant benefits across communities, there is inequitable exposure to harms including for Aboriginal and Torres Strait Islander families (eSafety Commissioner, 2023a), LGBTIQ+ people (eSafety Commissioner, 2021; Johns et al., 2022), culturally and linguistically diverse communities (eSafety Commissioner, 2020; Harris & Johns, 2021), and women (eSafety Commissioner, 2022c), with negative online experiences common across communities (eSafety Commissioner, 2022a), and young people keen for support in navigating eSafety (eSafety Commissioner, 2023b). Teachers face particular risk of harms, with high numbers reporting abuse (Burns et al., 2019; Williams, 2010) and emerging indications of new kinds of online abuse (Schultz, 2023).","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":120,"order":"118.00000000000000000000","Risk-challenge":"Justice and inequities in the burdens of data scraping and model outcomes: the burden of your data being scraped should not lead to inequitable outcomes in the benefits of models trained on that data. In a report commissioned by UNESCO in 2021, Johns et al. (2022) found that the safety of LGBTQ+ young people across the Asia Pacific region had been hampered by a rising tide of hate speech and misinformation during the COVID-19 pandemic, which targeted LGBTQ+ youth. Platform regulation, user tools and safety education designed to address these harms were regarded ambivalently by some young people in the study who spoke of the importance of social media in providing forums for community building and connection, while acknowledging that platform algorithms and business models also promote and exacerbate hate speech and harm (Johns et al., 2022, p. 30).","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":121,"order":"119.00000000000000000000","Risk-challenge":"Justice and inequities in access to opportunities to learn with and about genAI: Digital divides are comprised of inequities in affordability of devices and connectivity, accessibility across learners with diverse needs, and the abilities to engage effectively with these tools as both educators and learners. Australia has a significant digital divide, associated particularly with income and regional and remote location, and their intersection (Ingrid, 2020). Compounding concerns regarding exacerbation of digital divides is the challenge that those in regional and remote communities may have less access to professional learning opportunities, impacting capacity to support learning in schools.","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":122,"order":"120.00000000000000000000","Risk-challenge":"Accessibility and learning support: GenAI may also afford opportunities for supporting accessibility of materials, helping learners with disabilities (Marino et al., 2023). Potential uses include checking for structure and clarity of definitions, visual accessibility and genAI-automated modifications for different kinds of visual needs (e.g. variations in colour perception), video captioning, etc. (McMurtrie, 2023; Young & Maher, forthcoming). However, there is also potential for these tools to have different impacts on disabled students (Guo et al., 2020), for example through impacts on learning processes, and different intersections with existing learning needs, neither of which are currently well understood. Outside of formal learning contexts, there are also concerns that tools provided to support people may also expose that data inappropriately, for example in hiring processes that provide accessibility options, but without making it clear if that data will be visible to the hiring organisation (Wall & Schellmann, 2021). This concern of justice could exacerbate existing negative online experiences of people with intellectual disability (eSafety Commissioner, 2022b). Moreover, although there are some potentials in this space, caution must be taken that genAI does not substitute for appropriate resourcing to fulfil statutory obligations, particularly where the worth of the tools is not well established with respect to efficacy.","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":123,"order":"121.00000000000000000000","Risk-challenge":"Australian values: In the context of genAI itâ€™s also important to consider the particular values and context of Australia, which may not be well represented in the underlying data on which generative AI is trained, nor respect fundamental concerns of justice in Australia. While Nature has set out principles of genAI use in scholarship, as has COPE, they focus on authorship issues (Nature, 2023) rather than broader issues of justice in research. Uses of generative AI should not result in any diluting of information regarding Australia and our commitments and challenges. For example, there should be protections for Indigenous Cultural and Intellectual Property (ICIP). â€œThose who could be exploited by AI should be shaping its projectsâ€ (Kalluri, 2020).","Source":[{"id":19,"value":"19"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":124,"order":"122.00000000000000000000","Risk-challenge":"Equity in education is becoming increasingly linked with digital access. The need for greater equity remains a core concern for th e NT. Many NT students do not have access to basic reliable technology and the ability to consistently connect to the internet rem ains a priority with th e digital divide further disadvantaging a signific ant number of NT students. Innovativ e technology, such as generative Al, has a dependency on connectivity and digital capability of students and staff.","Source":[{"id":67,"value":"66"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":125,"order":"123.00000000000000000000","Risk-challenge":"As Al continues to advance, it is important to ensure this technology is culturally sensitive and inclusive of diverse perspectives and ex periences. This is espec ially true in the NT with a large representat ion of Aborigin a l students. Efforts are t ake n ac ross the department to ensure practices, r eso urce materials and policies are respectful of all cultures. The risks relating specifically to cultural sensitivity and Al, should be identified and mitigated wh ere possibl e to reduc e instances that may cause sadness or distress.","Source":[{"id":67,"value":"66"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":126,"order":"124.00000000000000000000","Risk-challenge":"Generative Al is based on existing data to generate new and original content. There is the risk of potential bias in the data sources which may result in unfair treatment of marginalised groups and perpetuate harmful stereotypes. Standards relating to data uses should be considered to ensure a fair representation to avoid bias in relation to aspects such as race, gender, language, politics and history.","Source":[{"id":67,"value":"66"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":127,"order":"125.00000000000000000000","Risk-challenge":"Knee-jerk bans on generative AI systems in public have the potential to produce an AI divide. If private and independent schools can teach their teachers and students how to get the most out of AI systems, while this is banned in the public system, two tiers of digital literacy will emerge. This will impact students beyond the primary and secondary school systems, putting public school graduates behind when they enter the workforce or further education. This two-tiered system will disproportionally impact those from disadvantaged backgrounds, with public students more than twice as likely to come from a disadvantaged background3, further entrenching their disadvantage. It must not be the case that only wealthy students from private schools are given the opportunity to develop AI literacy and gain the advantages that come with that.","Source":[{"id":14,"value":"14"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":128,"order":"126.00000000000000000000","Risk-challenge":"The fee-for-features model of many of the most popular generative AI services, where key features or priority processing is placed behind a paywall also has the potential to exacerbate inequalities in education. Schools and higher education providers will need to explore acquiring institutional licences for these products that can enable an equality of access for their students. Alternatively, education departments or higher education providers could develop their own generative AI tools using the GPT-44 (or similar generative AI models) Application Programming Interface (API) to create a custom environment specifically designed for learning and teaching (for example, the Khanmigo AI-powered learning system). Governments may also wish to consider providing access to more sophisticated existing US models, with the addition of gateways for the purposes of auditing student behaviour, and allowing teachers to access and monitor channels.","Source":[{"id":14,"value":"14"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":129,"order":"127.00000000000000000000","Risk-challenge":"The inclusion of generative AI within education systems will require rethinking of how assessments are conducted, presenting an opportunity to improve assessment practices. Educators have raised concerns that the rise of generative AI will lead to increased cheating and plagiarism and reduce the validity of assessment items. One solution is to take an archaic approach by returning assessments to supervised pen and paper examinations or oral examinations, that exclude the possibility of using AI systems. While this may be appealing and have a role in certain situations, this approach alone will likely fail to meet the needs of many students, rolling back many of the positive changes (like online learning) that have helped expand access to education, and would miss an opportunity to embrace this technological revolution. Instead, education providers should be supported to embrace and resource multiple forms of assessments for key learning outcomes, providing multiple opportunities and multiple methods of assessing studentâ€™s knowledge and abilities. Research shows that assessments that relate to authentic tasks, apply knowledge in a realistic context, and emphasise and assess a range of skills, are perceived by students to have long-term benefits and are reasonable in their demands as being positive for their learning (Struyven et al. 2006). The types of assessments to which generative AI systems may pose academic integrity risks (e.g. essays and traditional examinations) have previously received criticism for being poor measures of a studentsâ€™ knowledge and abilities (Rudolph et al. 2023). This presents an opportunity to reevaluate how Australian educational institutions at all levels can redesign assessments to allow them to not only survive in the AI age, but also work better to promote deep understanding and learning in Australian students.","Source":[{"id":14,"value":"14"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":130,"order":"128.00000000000000000000","Risk-challenge":"The widespread use of generative AI, particularly around student learning and performance also raises concerns around data privacy. These concerns include privacy of the underlying data upon which AI applications are trained, but also concerns around the use of information entered into these systems, particularly for AI systems that could help to provide educators feedback on how students are faring. Data privacy more broadly will be addressed by the ongoing consultation into the safe and responsible use of AI being conducted by the Department of Industry, Science and Resources, to which ATSE will be making a submission. Australian student data (such as contact details or grades) is currently stored internationally (for example via Google Classrooms), but given the nature of generative AI and emerging interaction patterns, the risk that very personal and potentially more risky data is stored about Australian students offshore is real and growing.","Source":[{"id":14,"value":"14"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":131,"order":"129.00000000000000000000","Risk-challenge":"As generative AI becomes increasingly sophisticated, it will be more difficult to ensure student interactions are focused on the intended learning area. There is an emerging risk that generative AI tools are interacting conversationally with users around mental health and wellbeing. This leads to risks that students may be encouraged to talk to an AI system rather than a human. While for some students discussing mental health issues with an AI may make them more comfortable to seek help for mental health issues, some students may be less likely to access timely interventions, might receive poor advice, or mental ill health may even be exacerbated by such interactions. Systems with high levels of human intervention will still be necessary to identify students at risk and intervene swiftly and appropriately.","Source":[{"id":14,"value":"14"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":132,"order":"130.00000000000000000000","Risk-challenge":"The rapid advancement of large language models is forcing educators to think carefully about what knowledge still needs to be taught when so much information can be so readily synthesised by AI. Some courses, particularly in tertiary education and specific disciplines, may quickly become outdated and require regular review and updates. This will only be enabled through the development of more agile systems of governance that are capable of being more responsive to changing context while upholding the integrity of the qualification. It is critical that the process of course design and review engages carefully setting the learning objectives, to ensure they are contemporary and appropriate in the age of AI, and will produce graduates with both discipline-expertise and the ability to use technology effectively and ethically.","Source":[{"id":33,"value":"33"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":133,"order":"131.00000000000000000000","Risk-challenge":"The emergence of AI tools capable of completing a substantial portion of traditional assessment tasks necessitates a re-evaluation of the purposes of assessment. While AI presents the opportunity to assess students on higher order cognitive skills, it is important to recognise that foundational content knowledge will continue to hold significance, especially in certain disciplines. It is crucial that the education sector develops new methods of assessment that can ensure learning outcomes in an age of AI tools to prevent an uncoupling of learning and assessment, which could have far-reaching consequences.","Source":[{"id":33,"value":"33"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":134,"order":"132.00000000000000000000","Risk-challenge":"The student/AI hybrid model also presents potential challenges for teachers, as they may need to assess the extent to which a student has modified the output of AI-generated work. This could increase the workload for teachers and require additional evaluation measures. While generative AI tools have the potential to revolutionize teaching and assessment practices, careful consideration of the purpose of education, the role of educators, and the evolving landscape of knowledge will be critical in harnessing the benefits of AI in a way that is inclusive and mitigates potential negative consequences. To effectively navigate an AIdependent environment, ongoing professional development will be essential for teachers, school support staff, administrative personnel, and policymakers.","Source":[{"id":33,"value":"33"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":135,"order":"133.00000000000000000000","Risk-challenge":"The increasing sophistication of AI poses diverse risks to research integrity, and TEQSA is engaging with the higher education sector to ensure that these risks are being recognised and reflected in updated institutional policy frameworks and the associated practices. AI has the capability to generate not just fake data, including false or doctored images, but entirely manufactured studies and journal articles. Images generated by AI are becoming increasingly difficult to detect and can compromise the integrity of research findings.","Source":[{"id":33,"value":"33"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":136,"order":"134.00000000000000000000","Risk-challenge":"Additionally, the administrative burden of the scientific peer-review process may result in reviewers outsourcing the review to AI systems to either provider the reviewer with a summary or provide feedback. AI systems do not hold the same level of detailed expertise of a human discipline expert, which may result in fewer erroneous or misleading research findings being identified prior to publication. Decreasing genuine engagement by experts with novel research in their field also presents a risk to innovation and collaboration","Source":[{"id":33,"value":"33"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":137,"order":"135.00000000000000000000","Risk-challenge":"There are significant risks to intellectual property where sensitive pre-published research findings, doctoral theses presented for examination or grant applications are uploaded to a third-party platform, and TEQSA is working with institutions to ensure they are proactively managing these risks through their policies and contracts.","Source":[{"id":33,"value":"33"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":138,"order":"135.00000000000000000000","Risk-challenge":"It not appropriately managed, AI has the potential to dilute the quality of published research, obscure genuine research in a sea of AI-generated content and ultimately undermine the publicâ€™s trust in the scientific process.","Source":[{"id":33,"value":"33"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":139,"order":"136.00000000000000000000","Risk-challenge":"The current crop of large language models can produce many of the artefacts that institutions have historically relied on for that assurance of learning, such as essays, coding tasks and both worded and numerical maths problems. As a result, without a transformation in how institutions assess student achievement of learning outcomes, there is a risk to the integrity of the system. While some commentators have welcomed the fact that AI can replace lower-level tasks that require content recall and allow for assessment of higher order cognition, there are several fundamental principles to consider. â€¢ The accuracy of current language models is not reliable, and their output needs to be scrutinised for errors. If the education system were to shift entirely to a â€œStudent/AI hybridâ€ model, it raises concerns about how future students will acquire the necessary content knowledge to effectively evaluate AI-generated output. â€¢ Completion of tasks such as essays holds pedagogical value that is unrelated to assessment purposes. For example, through writing essays, students learn to research, structure and defend arguments, and communicate their ideas in a written form. Motivating students to engage in these tasks may become challenging without the incentive of assessmentbased evaluation. Assessing the process of creating the artefact while employing the use of AI tools can provide an effective assessment process, but requires a greater investment of academic time in the marking process. These opportunities and impacts need to be recognised and appropriately resourced by institutions. â€¢ The use of AI in education raises serious concerns about data sovereignty and privacy. It is essential to address these issues and ensure that the use of AI tools does not compromise student, staff or institutional data and intellectual property. â€¢ Humanity will continue to require future generations of creative thinkers with discipline expertise. Because generative AI is trained on data, and all data is by definition historical, an over-reliance on AI may limit innovation, insight, and discovery. It is therefore crucial that society scaffolds in the introduction of AI tools through a studentâ€™s education journey, to ensure all students develop critical thinking skills and to defend the pipeline of students who can reach expert levels. â€¢ There is a risk of AI systems becoming self-contained and self-referential. An illustrative example is an educator using AI to write the lesson plans, design the assessment task and marking rubric. The student then uses AI to produce the assessment tasks, and the educator then uses AI to grade the assessment and provide feedback. In such a situation, the limited human involvement in the process undermines not just the educational experience but the very process of learning. Ultimately, AI is necessitating a rethink of how institutions of higher education can assure themselves that students have met the learning outcomes of a degree when the artefacts that have historically been relied on in current forms of assessment can be completed to a passing level with minimal student engagement.","Source":[{"id":33,"value":"33"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":140,"order":"137.00000000000000000000","Risk-challenge":"Current large language models are reflective of the enormous data sets they are trained on, resulting in a reflection of the biases and discrimination already present in those data sets. AI also has the potential to reinforce bias and disadvantage when algorithms are misused or poorly designed. Further, the risk of automation bias, with humans tending to place higher trust in an automated decision-making system, must be recognised and mitigated.","Source":[{"id":33,"value":"33"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":141,"order":"138.00000000000000000000","Risk-challenge":"23.\tWhile generative AI tools may be able to replace some of the tasks that are currently performed by teachers, this technology is best used to enhance teaching. It cannot replace the indispensable role of human interaction and cooperation, which must remain at the heart of education in Australia.\n24.\tThe 2023 UNESCO publication, Generative AI and the future of education, emphasised the need to â€˜be watchful against the potentials of newly powerful generative AI technology, alongside older digital tools and services, to undermine the authority and status of teachers â€¦â€™. It further noted that while â€˜frontier technologyâ€™ might be part of the answer to particular challenges in education, ultimately â€˜well-run schools, enough teachers, and teachers with the requisite conditions, training and salaries that allow them to be successful remain the main ingredients of a sustainable remedyâ€™.17","Source":[{"id":66,"value":"65"}],"Tags":[{"id":977965,"value":"de-professionalisation","color":"cyan"}],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":142,"order":"139.00000000000000000000","Risk-challenge":"25.\tThe right to privacy is a cornerstone human right18 and its importance â€˜in an increasingly data-centric world is growingâ€™,19 because generative AI tools may not only facilitate privacy intrusions, but deepen those intrusions in new and concerning ways.20 Generative AI tools rely on both large data sets to train the technology and the collection of personal data to optimize the individual user experience. This creates a range of privacy risks, particularly given the increased prevalence of cyberattacks and data breaches. The Commission has highlighted these issues in a number of submissions made this year.21 These concerns are particularly relevant in the context of using generative AI tools in the education system, where many of the users will be children who will have no real option but to use the technology if it is adopted by their schools.\n26.\tThere is an urgent need to ensure that generative AI tools are only adopted where concerns related to data privacy, security and consent are being considered and actively addressed.\n27.\tThe Attorney-Generalâ€™s Department is conducting a review of the Privacy Act 1988 (Cth), having closed submissions to their Review Report in April 2023. Several proposals in the Review Report considered strengthening privacy protections for children and in respect of AI. It is likely that outcomes from the Review Report will directly impact privacy, security and data protection for children and certain AI tools.\n28.\tThere must be clear guidelines established to expressly protect student data, limit access to sensitive information, and ensure that robust privacy and security measures are in place. Standards should be established to govern the collect, storage and use of personal information in the context of generative AI tools in education.","Source":[{"id":66,"value":"65"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":143,"order":"140.00000000000000000000","Risk-challenge":"29.\tA related risk that requires urgent attention is the potential commercialisation of student data obtained from the use of generative AI tools.\n \n\n\nAustralian Human Rights Commission\nUtilising Ethical AI in the Australian Education System, 14 July 2023\n\n\n30.\tPractices such as the sale or transfer of childrenâ€™s personal data to third parties should be banned, or heavily restricted, to protect childrenâ€™s rights. For example, among other things, General Comment 25 requires parties to:\n[P]rohibit by law the profiling or targeting of children of any age for commercial purposes on the basis of a digital record of their actual or inferred characteristics, including group or collective data, targeting by association or affinity profiling.\nPractices that rely on neuromarketing, emotional analytics, immersive advertising and advertising in virtual and augmented reality environments to promote products, applications and services should also be prohibited from engagement directly or indirectly with children.22\n31.\tOne example is the use of student search queries being analysed to inform targeted advertising. Another is for student data obtained via educational AI applications to be on-sold to third parties.23\n32.\tIn 2021 the National Childrenâ€™s Commissioner warned that by a childâ€™s 13th birthday advertisers will have gathered on average more than 72 million data points about them.24 It is essential that the data collected through the use of educational technology at schools should not be used for other purposes, and that children are protected from data surveillance.","Source":[{"id":66,"value":"65"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":144,"order":"141.00000000000000000000","Risk-challenge":"33.\tGenerative AI tools are trained on large datasets and generate predictive outputs based on algorithms. It is widely recognised that â€˜algorithms are not neutral, they are developed using metadata that exclude information on marginalized groups and are therefore unrepresentative or biasedâ€™.25\n34.\tThis means that generative AI tools can generate output that is biased, and potentially perpetuate unfairness or even result in unlawful discrimination.26 The risks of bias and discrimination are highlighted in the AI Discussion Paper27 and were examined in detail by the Commission in the Final Report and the Technical Paper, Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias. It is essential to ensure that this issue is addressed so that Australiaâ€™s education system is fair, inclusive and promotes equal opportunities for all students.","Source":[{"id":66,"value":"65"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":145,"order":"142.00000000000000000000","Risk-challenge":"35.\tThe AI Discussion Paper highlighted the creation of misinformation and disinformation as one of the key challenges posed by the increased application of AI.28 The Rapid Response Information Report commissioned by Australiaâ€™s National Science and Technology Council similarly identified both large language models (LLMs) and multimodal foundation models (MFMs) as having â€˜the potential for misuse by generating high-quality, cheap and personalised content, including for harmful purposesâ€™, with the use of generative AI tools to generate deep fakes.29\n36.\tThe use of generative AI tools in educational settings necessarily raises ethical considerations in terms of their potential use to create fake or manipulated content, such as â€˜deepfakesâ€™.30","Source":[{"id":66,"value":"65"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":146,"order":"143.00000000000000000000","Risk-challenge":"37.\tIn isolation, these reforms are insufficient. There is also a need for digital literacy education and training to ensure that users are able to identify fake or manipulated content, and establish that users understand the importance of engaging with the technology in responsible and ethical ways. The importance of promoting digital literacy is discussed further below.","Source":[{"id":66,"value":"65"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":147,"order":"144.00000000000000000000","Risk-challenge":"38.\tThe ease with which essays, research papers or creative works can be produced using generative AI tools creates risks of plagiarism or intellectual dishonesty. The\n \n\n\nAustralian Human Rights Commission\nUtilising Ethical AI in the Australian Education System, 14 July 2023\n\n\nrelease of ChatGPT in November last year immediately gave rise to concerns â€˜that\na tsunami of cheating was on the horizonâ€™.31\n39.\tBoth around the world, and in Australia, many schools and universities responded by initially banning these technologies from their devices and networks. The New York City public school district became one of the first to temporarily ban ChatGPT from its schoolsâ€™ devices and networks in January 2023, although this ban was reversed several months later.32 Many schools and universities in Australia have also introduced bans or restrictions on the use of generative AI tools at various points, including initial bans by the public school systems in all Australian states other than South Australia.33\n40.\tThere is now growing recognition that an absolute ban on this technology is likely unworkable and disadvantageous for students.34 In particular any ban that is not consistently applied across the Australian education system will create a digital divide between those who have been taught to use generative AI tools at school, and those who have not.35 It also forgoes an important opportunity to teach students how to responsibly use new technologies that are becoming increasingly important, and to develop necessary skills for the future.\n41.\tInstead we should aim to encourage the responsible and ethical use of generative AI tools by students, rather than simply banning them. Students should be educated about the importance of academic integrity, and clear guidelines should be established on the appropriate use of AI-generated content, citation practices, and the need for originality in student work.\n42.\tTo ensure academic integrity, there must be greater research, development and deployment of digital tools capable of identifying AI-generated content. One example is the development of Checker AI which uses its own AI models to predict if text has been written by a human, and to verify the authenticity of student work. This demonstrates the duality of AI tools being both a cause, and solution, to challenges in Australiaâ€™s education system.","Source":[{"id":66,"value":"65"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":148,"order":"145.00000000000000000000","Risk-challenge":"49.\tWhile generative AI has the potential to improve educational outcomes, it is critical to address the digital divide and ensure that equitable opportunities are created for all students, regardless of their background.\n50.\tThe 2021 Australian Digital Inclusion Index shows that there remains a substantial digital divide in Australia.36 One in four people in Australia were identified as being â€˜digitally excludedâ€™ and â€˜people with low levels or income, education and\n \n\n\nAustralian Human Rights Commission\nUtilising Ethical AI in the Australian Education System, 14 July 2023\n\n\nemployment, those living in some regional areas, people aged over 65 and people\nwith a disabilityâ€™ being identified as being of particular risk of being left behind.37\n51.\tStudents who are unable to access the digital tools they need for school risk missing out on crucial learning opportunities that other students take for granted, with the risk of exacerbating educational disadvantage. The Smith Family has estimated that 1 in 6 of the families they work with cannot currently access the digital tools that their children need for school.38 Research published by the Australian Education Union in 2020 revealed a â€˜persistent long-term gap in digital access, affordability and ability experienced by many public school studentsâ€™.39 Ensuring digital inclusion and equity needs to be a key principle informing the use of generative AI in the Australian education system.\n52.\tGenerative AI tools can also potentially play a significant role in helping to level the playing field for all students, regardless of their backgrounds. These tools can provide access to high-quality educational content to students from disadvantaged or low socio-economic backgrounds, and the potential for AI- powered tutors and translators to assist students with particular needs or challenges is significant.\n53.\tIn order to both improve digital equity in the use of generative AI and harness the potential benefits of generative AI tools in reducing educational disadvantage, policies that remove barriers to access, provide targeted training and capacity building, and encourage community engagement and outreach should be pursued.","Source":[{"id":66,"value":"65"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":149,"order":"146.00000000000000000000","Risk-challenge":"54.\tEnsuring equitable access to technology for students is essential to help close the digital divide. This means that public schools must be resourced to allow them to provide students with access to technology, and also ensuring that digital technology is available for use in community facilities (such as libraries).\n55.\tCollaborations between government agencies, educational institutions, not-for- profit organisations and the private sector are crucial for ensuring access to resources to benefit disadvantaged cohorts.","Source":[{"id":66,"value":"65"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":150,"order":"147.00000000000000000000","Risk-challenge":"","Source":[],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":151,"order":"148.00000000000000000000","Risk-challenge":"","Source":[],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":152,"order":"149.00000000000000000000","Risk-challenge":"","Source":[],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":153,"order":"150.00000000000000000000","Risk-challenge":"","Source":[],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":154,"order":"151.00000000000000000000","Risk-challenge":"","Source":[],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":155,"order":"152.00000000000000000000","Risk-challenge":"","Source":[],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":156,"order":"153.00000000000000000000","Risk-challenge":"digital divide","Source":[{"id":48,"value":"48"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":157,"order":"154.00000000000000000000","Risk-challenge":"academic integrity","Source":[{"id":48,"value":"48"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":158,"order":"155.00000000000000000000","Risk-challenge":"bias and discrimination","Source":[{"id":48,"value":"48"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":159,"order":"156.00000000000000000000","Risk-challenge":"data privacy and transparency","Source":[{"id":48,"value":"48"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":160,"order":"157.00000000000000000000","Risk-challenge":"safety and wellbeing (e.g. deepfakes, exploitation of data for commercial gain..)","Source":[{"id":48,"value":"48"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":161,"order":"158.00000000000000000000","Risk-challenge":"bias and ethical harm","Source":[{"id":49,"value":"49"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":162,"order":"159.00000000000000000000","Risk-challenge":"inaccurate or false information","Source":[{"id":49,"value":"49"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":163,"order":"160.00000000000000000000","Risk-challenge":"privacy and intellectual property violations","Source":[{"id":49,"value":"49"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":164,"order":"161.00000000000000000000","Risk-challenge":"opacity and unexplainability","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":165,"order":"162.00000000000000000000","Risk-challenge":"data privacy and security","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":166,"order":"163.00000000000000000000","Risk-challenge":"personalisation and fairness","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":167,"order":"164.00000000000000000000","Risk-challenge":"effectiveness and reliability","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":168,"order":"165.00000000000000000000","Risk-challenge":"Lack of indicators of fabricated sources","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":169,"order":"166.00000000000000000000","Risk-challenge":"Blindness to the relationality of sources","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":170,"order":"167.00000000000000000000","Risk-challenge":"Impeded traceability to original content creators and authors","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":171,"order":"168.00000000000000000000","Risk-challenge":"Obfuscation of copyright conditions","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":172,"order":"169.00000000000000000000","Risk-challenge":"Legal and integrity risks related to consent, compensation, cultural sensitivity, and output quality","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":173,"order":"170.00000000000000000000","Risk-challenge":"Uncertainty about the integrity of Generative AI tools and copyright ramifications for teaching practice","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":174,"order":"171.00000000000000000000","Risk-challenge":"Potential increase in costs for schools accessing Generative AI tools","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":175,"order":"172.00000000000000000000","Risk-challenge":"Restructuring of access to copyrighted content through micropayments and platform-based deals","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":176,"order":"173.00000000000000000000","Risk-challenge":"Data degradation over time due to the use of mixed original and AI-generated data","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":177,"order":"174.00000000000000000000","Risk-challenge":"Risk of algorithmic bias and underrepresentation of students from diverse backgrounds","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":178,"order":"175.00000000000000000000","Risk-challenge":"Lack of cultural understanding and reproduction of colonialist paradigms","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":179,"order":"176.00000000000000000000","Risk-challenge":"Risk of isolation or invisibility for First Nations students and the need for cultural visibility protocols","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":180,"order":"177.00000000000000000000","Risk-challenge":"Augmentation of online abuse and challenges in identification and moderation","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":181,"order":"178.00000000000000000000","Risk-challenge":"Ownership and consent issues regarding human-generated data","Source":[{"id":92,"value":"91"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":182,"order":"179.00000000000000000000","Risk-challenge":"Potential for data leakage","Source":[{"id":91,"value":"90"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":183,"order":"180.00000000000000000000","Risk-challenge":"Potential for inaccurate information","Source":[{"id":91,"value":"90"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":184,"order":"181.00000000000000000000","Risk-challenge":"Potential for harmful content","Source":[{"id":91,"value":"90"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":185,"order":"182.00000000000000000000","Risk-challenge":"Potential for bias in generated content","Source":[{"id":91,"value":"90"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":186,"order":"183.00000000000000000000","Risk-challenge":"AI technologies \ncan potentially be used in ways that compromise \nindividual privacy, perpetuate biases, or lead to \nunethical outcomes. Lacking appropriate guidelines \nmay result in unintended consequences, such as the \nmisuse of AI-generated content or assessments","Source":[{"id":89,"value":"88"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":187,"order":"184.00000000000000000000","Risk-challenge":"The lack of it may \nlead to a lack of trust and understanding among \neducators and students. If users cannot comprehend \nhow AI-generated content is produced, they may be \nhesitant to adopt AI tools or question their \nreliability.","Source":[{"id":89,"value":"88"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":188,"order":"185.00000000000000000000","Risk-challenge":"Generative AI tools trained on \nbiased data can perpetuate those biases in the \ncontent they generate, leading to unfair or \ndiscriminatory educational experiences. Addressing \nand mitigating bias is a crucial challenge in ensuring \nequitable access to quality education for all \nstudents.","Source":[{"id":89,"value":"88"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":189,"order":"186.00000000000000000000","Risk-challenge":"While AI can enhance educational \nexperiences, excessive reliance on AI without human \noversight poses a risk. Human educators play a \ncritical role in reviewing and validating AI-generated \ncontent to ensure accuracy, appropriateness, and \nalignment with educational goals.","Source":[{"id":89,"value":"88"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":190,"order":"187.00000000000000000000","Risk-challenge":"The risk of irresponsible data \nuse arises when AI tools collect and process student \ndata without adequate safeguards. Mismanagement \nof data can lead to privacy breaches, misuse of \ninformation, or unauthorized access, compromising the trust between educational institutions and \nstakeholders","Source":[{"id":89,"value":"88"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":191,"order":"188.00000000000000000000","Risk-challenge":"The integration of AI \nin education requires robust security measures to \nprotect AI models and educational data from cyber \nthreats. Failing to implement proper security \nprotocols can lead to data breaches, compromising \nstudent privacy and the integrity of the education \nsystem.","Source":[{"id":89,"value":"88"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":192,"order":"189.00000000000000000000","Risk-challenge":"With the introduction of \nAI in education, the risk of academic misconduct \nmay evolve, including AI-generated content used for \nplagiarism or cheating. Establishing academic \nintegrity policies to address these new challenges is\nessential to maintain the credibility and \ntrustworthiness of educational institutions.","Source":[{"id":89,"value":"88"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":193,"order":"190.00000000000000000000","Risk-challenge":"The lack of rigorous \nvalidation of AI tools for education can lead to the \nadoption of ineffective or unreliable tools. Proper \nresearch and validation are essential to understand \nthe impact of AI on learning outcomes and ensure \nthat educators make informed decisions about \nintegrating AI in classrooms.","Source":[{"id":89,"value":"88"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":194,"order":"191.00000000000000000000","Risk-challenge":"Educators, students, and \nstakeholders need to be aware of the opportunities \nand challenges posed by generative AI tools in \neducation. Lack of education and awareness can \nhinder the responsible and effective integration of AI \nin classrooms","Source":[{"id":89,"value":"88"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":195,"order":"192.00000000000000000000","Risk-challenge":"Addressing the complex challenges of AI in \neducation requires a collaborative effort from \nexperts in education, AI development, ethics, and \npolicy. A lack of collaboration can result in \nfragmented approaches and limited perspectives on \nethical and practical considerations.","Source":[{"id":89,"value":"88"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":196,"order":"193.00000000000000000000","Risk-challenge":"Concerns about evaluating student work when AI is used in healthcare education","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":197,"order":"194.00000000000000000000","Risk-challenge":"Impact of AI on future professionals' clinical decision-making skills and professional language of nursing","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":198,"order":"195.00000000000000000000","Risk-challenge":"Challenges of incorporating digital literacy into an already stretched health curriculum","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":199,"order":"196.00000000000000000000","Risk-challenge":"Academic integrity challenges, including plagiarism and reliance on misleading or biased information when using AI","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":200,"order":"197.00000000000000000000","Risk-challenge":"Difficulties in detecting AI use in formulating assessments and identifying cases of plagiarism","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":201,"order":"198.00000000000000000000","Risk-challenge":"Inadequacy of existing detection tools like Turnitin in keeping up with AI tools","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":202,"order":"199.00000000000000000000","Risk-challenge":"Need for developing more responsive detection tools for identifying plagiarism","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":203,"order":"200.00000000000000000000","Risk-challenge":"Development of educational policies surrounding the use of AI in higher education","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":204,"order":"201.00000000000000000000","Risk-challenge":"Redesigning assessments to ensure students provide drafts to benchmark their work","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":205,"order":"202.00000000000000000000","Risk-challenge":"Additional costs and labor for educational organizations to implement interventions addressing AI-related challenges","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":206,"order":"203.00000000000000000000","Risk-challenge":"Risks of relying on AI shortcuts for obtaining a clinical education, potentially endangering patient care","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":207,"order":"204.00000000000000000000","Risk-challenge":"Concerns about the quality of information produced by AI interventions","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":208,"order":"205.00000000000000000000","Risk-challenge":"Reliability issues with current data retrieval due to the need for integration between healthcare digital systems","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":209,"order":"206.00000000000000000000","Risk-challenge":"Requirement of significant investment in national digital infrastructure for wider use of generative AI in educational systems.","Source":[{"id":88,"value":"87"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":210,"order":"207.00000000000000000000","Risk-challenge":"potentially providing incorrect, biased, or fake information. It may also potentially result in \nbreaches of privacy. Other issues include potential breaches of copyright and risks of \nplagiarism","Source":[{"id":87,"value":"86"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":211,"order":"208.00000000000000000000","Risk-challenge":"One possible socio-economic ethical risk is the possibility that persons of higher income or \ndisposable assets could pay for access to generative AI services, whereas persons of lesser \nmeans would be unable to do so. In such a scenario, persons of higher means would clearly \nbe advantaged â€“ possibly in breach of rules around academic integrity (so-called AI\u0002assisted cheating). Consideration should be given as to the socio-economic challenges of \npotentially entrenching, or increasing, inequality in education and our society generally.","Source":[{"id":87,"value":"86"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":212,"order":"209.00000000000000000000","Risk-challenge":"Socio-economic, and socio-cultural ethical challenges of generative AI ought to be \nconsidered by the Australian government.","Source":[{"id":87,"value":"86"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":213,"order":"210.00000000000000000000","Risk-challenge":"The gradual advancement of generative AI could change the roles and responsibilities of \neducators and students, and perhaps, even the concept of schooling, academic qualification,\nand certification. For instance, technological advancement could possibly change the role \nof teachers from educators to facilitators.","Source":[{"id":87,"value":"86"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":215,"order":"212.00000000000000000000","Risk-challenge":"online abuse involving the education workforce.","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":216,"order":"213.00000000000000000000","Risk-challenge":"online safety","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":217,"order":"214.00000000000000000000","Risk-challenge":"being filmed \nwithout consent","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":218,"order":"215.00000000000000000000","Risk-challenge":"sexual abuse","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":219,"order":"216.00000000000000000000","Risk-challenge":"digitally \nmanipulated image-based abuse","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":220,"order":"217.00000000000000000000","Risk-challenge":"AI technology may be used \nto create more sophisticated and realistic images, or do so at larger scale, as the \ntechnology becomes more widespread","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":221,"order":"218.00000000000000000000","Risk-challenge":"Generative AI tools may allow bad actors with malicious intent to create authentic looking, \nhigh quality deepfakes of individuals without requiring this repository of images to build on","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":222,"order":"219.00000000000000000000","Risk-challenge":"age-inappropriate conversations or content, including sexual or violent content, when \nchildren and young people engage with a chatbot. This includes where no age assurance \nis present, where a user can enter an incorrect age, or where the display of content is \ninadvertent (e.g., a failure of an age estimation algorithm).","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":223,"order":"220.00000000000000000000","Risk-challenge":"the potential for chatbots to be used as a tool for \ngrooming by starting conversations through social media or gaming platforms to manipulate \nchildren and young people.","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":224,"order":"221.00000000000000000000","Risk-challenge":"a related issue concerns the capacity of chatbots to appropriately identify, respond to \nand report concerns for the safety and welfare of children and young people.","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":225,"order":"222.00000000000000000000","Risk-challenge":"produce â€˜human-likeâ€™ interaction combined with novel high \nquality personalised content could lead to an amplification of existing cyberbullying and \ncyber abuse harms","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":226,"order":"223.00000000000000000000","Risk-challenge":"certain groups are especially vulnerable to the impact of online harms and experience \nhigher rates of online targeted abuse (Aboriginal and Torres Strait Islander children and young people)","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":227,"order":"224.00000000000000000000","Risk-challenge":"replicate any existing stereotypes, biases and discriminatory \nviewpoints contained in their training datasets","Source":[{"id":85,"value":"84"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":228,"order":"225.00000000000000000000","Risk-challenge":"concerns about data privacy \nand security regarding sensitive student information, parallel to similar challenges faced in recent years \nwith the integration of education technology into the classroom.","Source":[{"id":84,"value":"83"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":229,"order":"226.00000000000000000000","Risk-challenge":"put childrenâ€™s rights at risk, monitoring them without their consent and allowing access from \nor selling the data to third parties","Source":[{"id":84,"value":"83"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":230,"order":"227.00000000000000000000","Risk-challenge":"making children particularly vulnerable to content and \nmicrotargeted marketing from brands trying to sell dangerous products such as alcohol, vapes or \ngambling","Source":[{"id":84,"value":"83"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":231,"order":"228.00000000000000000000","Risk-challenge":"it can cause severe anxiety, \na culture of distrust and symptoms similar to post-traumatic stress disorder","Source":[{"id":84,"value":"83"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":232,"order":"229.00000000000000000000","Risk-challenge":"Generative AI tools, like some EdTech applications, continuously gather data exposing children to the \nsame risks.","Source":[{"id":84,"value":"83"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":233,"order":"230.00000000000000000000","Risk-challenge":"Generative AI models use machine learning models (MLM) which are trained using large datasets to \nperform a programmed function. If the training data predominantly represents a specific demographic, \nculture, or perspective, the generated output may reflect those biases and further entrench and exacerbate\ninequalities and stereotypes present in our societies","Source":[{"id":84,"value":"83"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":234,"order":"231.00000000000000000000","Risk-challenge":"risks to \nthe learner-teacher relationship","Source":[{"id":82,"value":"81"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":235,"order":"232.00000000000000000000","Risk-challenge":"the ethical use of data and assessment results","Source":[{"id":82,"value":"81"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":236,"order":"233.00000000000000000000","Risk-challenge":"the data privacy \nand security of data using AI","Source":[{"id":82,"value":"81"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":237,"order":"234.00000000000000000000","Risk-challenge":"a challenge for ongoing academic and research integrity","Source":[{"id":82,"value":"81"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":238,"order":"235.00000000000000000000","Risk-challenge":"threat to using some forms of essays for assessment and for online exams","Source":[{"id":82,"value":"81"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":239,"order":"236.00000000000000000000","Risk-challenge":"reduces the social aspect of \nlearning and the agency of educators","Source":[{"id":82,"value":"81"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":240,"order":"237.00000000000000000000","Risk-challenge":"AI lacks an \nunderstanding of the context of learning","Source":[{"id":82,"value":"81"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":241,"order":"238.00000000000000000000","Risk-challenge":"o become a distraction and reduce a productive focus on \neffective teaching and learning","Source":[{"id":82,"value":"81"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":242,"order":"239.00000000000000000000","Risk-challenge":"Over reliance on AI may reduce opportunities for critical thinking and problem solving, and \ncreativity","Source":[{"id":82,"value":"81"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":243,"order":"240.00000000000000000000","Risk-challenge":"Consent and Control","Source":[{"id":79,"value":"78"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":244,"order":"241.00000000000000000000","Risk-challenge":"Lack of Transparency","Source":[{"id":79,"value":"78"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":245,"order":"242.00000000000000000000","Risk-challenge":"Impact on Australian music creators","Source":[{"id":79,"value":"78"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":246,"order":"243.00000000000000000000","Risk-challenge":"Authorship and intellectual property concerns","Source":[{"id":78,"value":"77"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":247,"order":"244.00000000000000000000","Risk-challenge":"Confidentiality and privacy of information, and integrity in the peer review process","Source":[{"id":78,"value":"77"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":248,"order":"245.00000000000000000000","Risk-challenge":"use of generative AI in the conduct of research; Australiaâ€™s research integrity framework","Source":[{"id":78,"value":"77"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":251,"order":"248.00000000000000000000","Risk-challenge":"For students, relying on GAI may provide answers which lack depth, nuance or are just incorrect.","Source":[{"id":77,"value":"76"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":252,"order":"249.00000000000000000000","Risk-challenge":"If relied on without appropriate scaffolding, it could reduce opportunities to learn key research and writing skills which are fundamental building blocks for tertiary education.","Source":[{"id":77,"value":"76"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":253,"order":"250.00000000000000000000","Risk-challenge":"Some academics remain fearful that the teaching profession will be devalued by the use of GAI or may eventually be rendered obsolete","Source":[{"id":77,"value":"76"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":254,"order":"251.00000000000000000000","Risk-challenge":"Biased data","Source":[{"id":76,"value":"75"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":255,"order":"252.00000000000000000000","Risk-challenge":"Inaccuracy of output and promoting disinformation","Source":[{"id":76,"value":"75"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":256,"order":"253.00000000000000000000","Risk-challenge":"Privacy violations","Source":[{"id":76,"value":"75"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":257,"order":"254.00000000000000000000","Risk-challenge":"Diminishing face to face interaction","Source":[{"id":76,"value":"75"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":258,"order":"255.00000000000000000000","Risk-challenge":"Threat to academic integrity","Source":[{"id":76,"value":"75"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":259,"order":"256.00000000000000000000","Risk-challenge":"A key risk that academics are concerned about with Generative AI is the ease with\nwhich students can use it to generate answers to questions, rather than thinking about\nthe answers themselves. Especially when they are being assessed.","Source":[{"id":74,"value":"74"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":260,"order":"257.00000000000000000000","Risk-challenge":"AI systems may discriminate in ways that are without precedent\nand that there are currently few means of detecting or investigating this to prevent discrimination","Source":[{"id":72,"value":"72"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":261,"order":"257.00000000000000000000","Risk-challenge":"this can hinder the collection of evidence to mount\na prima facie case for new forms of discrimination","Source":[{"id":72,"value":"72"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":262,"order":"258.00000000000000000000","Risk-challenge":"AI will be\nradically disruptive to career education generally including the possible decimation of the career\npipeline for creative industries","Source":[{"id":72,"value":"72"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":263,"order":"259.00000000000000000000","Risk-challenge":"humans anthropomorphise and can overly trust machines","Source":[{"id":72,"value":"72"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":264,"order":"260.00000000000000000000","Risk-challenge":"Risks in ensuring safe and ethical use of generative AI tools","Source":[{"id":71,"value":"71"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":265,"order":"261.00000000000000000000","Risk-challenge":"Challenges in promoting ongoing academic and research integrity","Source":[{"id":71,"value":"71"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":266,"order":"262.00000000000000000000","Risk-challenge":"Dependence on big technology companies for public education","Source":[{"id":71,"value":"71"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":267,"order":"263.00000000000000000000","Risk-challenge":"Lack of transparent and accountable models in AI implementation","Source":[{"id":71,"value":"71"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":268,"order":"264.00000000000000000000","Risk-challenge":"Concerns regarding equitable access to leading-edge technology for all students","Source":[{"id":71,"value":"71"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":269,"order":"265.00000000000000000000","Risk-challenge":"Privileged individuals retaining access to advanced tools","Source":[{"id":71,"value":"71"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":270,"order":"266.00000000000000000000","Risk-challenge":"Data privacy concerns when uploading assessments to cloud or sharing with commercial bot detection services","Source":[{"id":70,"value":"70"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":271,"order":"267.00000000000000000000","Risk-challenge":"Ethical issues surrounding the use of bot detection services, including potential bias and false-positive rates","Source":[{"id":70,"value":"70"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":272,"order":"268.00000000000000000000","Risk-challenge":"Challenges in ensuring academic integrity without changing assessment tasks to account for generative AI availability","Source":[{"id":70,"value":"70"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":273,"order":"269.00000000000000000000","Risk-challenge":"Tutory, a ChatGPT plugin designed to answer students' academic questions, raising further concerns about academic integrity","Source":[{"id":70,"value":"70"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":274,"order":"270.00000000000000000000","Risk-challenge":"Challenges in assessing student performance given ChatGPT's ability to excel in certain assessment types","Source":[{"id":70,"value":"70"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":275,"order":"271.00000000000000000000","Risk-challenge":"Difficulty in devising perfect tools to detect AI plagiarism, cautioning against over-reliance on plagiarism detection tools","Source":[{"id":70,"value":"70"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":276,"order":"272.00000000000000000000","Risk-challenge":"AI is trained by or from materials\nthat are protected by copyright. Currently, this training ostensibly involves the copying of those\nmaterials without the permission of the copyright owners","Source":[{"id":69,"value":"69"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":277,"order":"273.00000000000000000000","Risk-challenge":"the content generated by generative AI is unlikely to be protected by copyright (because of\nthe lack of a human author)","Source":[{"id":69,"value":"69"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":278,"order":"274.00000000000000000000","Risk-challenge":"issues of copyright infringement and liability if the content generated by AI reproduces a\nâ€˜substantial partâ€™ of existing material that is protected by copyright.","Source":[{"id":69,"value":"69"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":279,"order":"275.00000000000000000000","Risk-challenge":"Perpetuation of biases in generative AI, potentially inherited from training data","Source":[{"id":68,"value":"68"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":280,"order":"276.00000000000000000000","Risk-challenge":"Threat to academic and research integrity by encouraging academic misconduct","Source":[{"id":68,"value":"68"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":281,"order":"277.00000000000000000000","Risk-challenge":"Difficulty in striking a balance between AI-generated material and original work to uphold scholarly standards","Source":[{"id":68,"value":"68"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":282,"order":"278.00000000000000000000","Risk-challenge":"Increasing difficulty in detecting AI-generated work as technology improves","Source":[{"id":68,"value":"68"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":283,"order":"279.00000000000000000000","Risk-challenge":"Lack of fully reflected implications of generative AI in university policies","Source":[{"id":68,"value":"68"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":284,"order":"280.00000000000000000000","Risk-challenge":"the limitations of generative AI systems, including accuracy and biases in training data","Source":[{"id":50,"value":"50"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":285,"order":"281.00000000000000000000","Risk-challenge":"the quality of the output of these models\ndepends on the quality of the input they were trained on. Widely available material that reflects public opinion but\ncontradicts scientific evidence or more recent evidence can contribute to uninformed responses","Source":[{"id":50,"value":"50"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":286,"order":"282.00000000000000000000","Risk-challenge":"Legal risks and challenges:Terms and conditions applicable to these tools may preclude the use by certain agegroups (e.g., under 13s) or require parental consent where a person is under 18","Source":[{"id":13,"value":"13"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":287,"order":"283.00000000000000000000","Risk-challenge":"Companies providing generative AI tools can reserve the right to change terms and\nconditions from time to time, often without notice.","Source":[{"id":13,"value":"13"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":288,"order":"284.00000000000000000000","Risk-challenge":"Use of generative AI tools in teaching or assessment may present issues if a student\n(or parent/carer, where consent is required) is not willing to accept the applicable\nterms and conditions, privacy policy, and so on.","Source":[{"id":13,"value":"13"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":289,"order":"285.00000000000000000000","Risk-challenge":"Inputting content into a generative AI tool could constitute an infringement of\ncopyright or intellectual property rights","Source":[{"id":13,"value":"13"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":296,"order":"292.00000000000000000000","Risk-challenge":"Helping children judge the veracity of content","Source":[{"id":13,"value":"13"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":297,"order":"293.00000000000000000000","Risk-challenge":"The risk of plagiarism","Source":[{"id":13,"value":"13"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":298,"order":"294.00000000000000000000","Risk-challenge":"Reinforcing socio-economic barriers and disadvantages","Source":[{"id":13,"value":"13"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":299,"order":"295.00000000000000000000","Risk-challenge":"generative AI may draw from datasets that harvest artistic and intellectual works without permission or\nacknowledgement, denying the rights of and threatening the jobs of those who create this work\nboth now and into the future","Source":[{"id":15,"value":"15"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":300,"order":"296.00000000000000000000","Risk-challenge":"generative AI may be implemented by corporations and other bodies in ways that are profoundly unethical, for\nexample, through labour exploitation or data manipulation","Source":[{"id":15,"value":"15"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":301,"order":"297.00000000000000000000","Risk-challenge":"generative AI may exacerbate the threats already presented within a digital society: including the spread of\nmisinformation at scale, unacceptable environmental costs through computational requirements\nand the rising inequality of wealth distribution","Source":[{"id":15,"value":"15"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":302,"order":"298.00000000000000000000","Risk-challenge":"Students who lack the skills to use such models will be severely disadvantaged when they enter the workforce. To remain competitive in the workforce, students must learn how to effectively use LLMs, even if just effectively prompting them to get a good output and assessing the outputâ€™s quality, accuracy, and originality","Source":[{"id":21,"value":"21"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":303,"order":"299.00000000000000000000","Risk-challenge":"people from culturally and linguistically diverse backgrounds as well as those with lower household income and lower levels of formal education are less likely to know about the tool and to use it for different purposesAs a result, EAL/D learners, both adults and children, are most likely to miss out on important language learning opportunities which have been widely documented in the research literature: oral and written conversational speaking practice; correcting errors in grammar; providing writing assistance; enriching vocabulary; development of fluency in pronunciation; personalised tutoring and feedback; enhancement of language learning motivation","Source":[{"id":21,"value":"21"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":304,"order":"300.00000000000000000000","Risk-challenge":"EAL/D learners, both adults and children, are most likely to miss out on a valuable tool which can significantly assist their settlement experiences through translation and simplifying information","Source":[{"id":21,"value":"21"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":305,"order":"301.00000000000000000000","Risk-challenge":"EAL/D learners, both adults and children, are most likely to miss out on opportunities to develop AI literacies, which involve the diverse skills, knowledge, understandings and attitudes required for meaningful, creative and critical use of generative AI for different purposes and in different contexts","Source":[{"id":21,"value":"21"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":306,"order":"302.00000000000000000000","Risk-challenge":"A new kind of \"digital divideâ€ is in process","Source":[{"id":21,"value":"21"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":307,"order":"303.00000000000000000000","Risk-challenge":"Potential exacerbation of existing inequities in access to education and technology","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":308,"order":"304.00000000000000000000","Risk-challenge":"Risk of unethical human use of generative AI, including the creation of deep fakes and plagiarism","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":309,"order":"305.00000000000000000000","Risk-challenge":"Algorithmic bias in generative AI software favoring certain ideologies and perspectives","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":310,"order":"306.00000000000000000000","Risk-challenge":"Difficulty in detecting relevance and accuracy in generated AI products","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":311,"order":"307.00000000000000000000","Risk-challenge":"Inaccurate or incomplete information being considered as accurate by users","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":312,"order":"308.00000000000000000000","Risk-challenge":"Potential for generative AI to be used as a single source of truth without applying a critical lens","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":313,"order":"309.00000000000000000000","Risk-challenge":"Concerns about assessment integrity and the need for advanced anti-plagiarism tools","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":314,"order":"310.00000000000000000000","Risk-challenge":"Potential increase in educator workload when incorporating generative AI tools","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":315,"order":"311.00000000000000000000","Risk-challenge":"Rapid pace of AI development requiring ongoing training and support for educators","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":316,"order":"312.00000000000000000000","Risk-challenge":"Financial investment considerations and the risk of rapid technological advancements superseding current versions of generative AI","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":317,"order":"313.00000000000000000000","Risk-challenge":"Generative AI tools have the potential to bridge the educational divide that impacts students and\nfamilies experiencing disadvantage.","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":318,"order":"314.00000000000000000000","Risk-challenge":"the cost and equitable access of generative AI tools so as not to\nexacerbate the digital divide. It is highly likely that superior versions of generative AI will be more\nexpensive","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":319,"order":"315.00000000000000000000","Risk-challenge":"disadvantaged students are  marginalised through lack\nof access to the same educational tools, including generative AI, as their peers.","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":320,"order":"316.00000000000000000000","Risk-challenge":"Disadvantaged communities may lack the necessary infrastructure and resources required to access,\nimplement and maintain relevant education technologies","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":321,"order":"316.00000000000000000000","Risk-challenge":"Indiscriminate implementation and use, without appropriate\nsafeguarding structures around accountability and quality control may prove problematic and amplify\ndisadvantage in some communities.","Source":[{"id":22,"value":"22"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":322,"order":"317.00000000000000000000","Risk-challenge":"Potential reinforcement of biases and stereotypes present in the data used for training","Source":[{"id":25,"value":"25"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":323,"order":"318.00000000000000000000","Risk-challenge":"Risks to fairness and exacerbation of discrimination and systemic inequalities","Source":[{"id":25,"value":"25"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":324,"order":"318.00000000000000000000","Risk-challenge":"Concerns about the safety and privacy of personal data used by AI tools","Source":[{"id":25,"value":"25"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":325,"order":"319.00000000000000000000","Risk-challenge":"Need for robust privacy policies, security measures, and compliance with data protection laws","Source":[{"id":25,"value":"25"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":326,"order":"320.00000000000000000000","Risk-challenge":"Potential for plagiarism and unethical use of AI-generated content","Source":[{"id":25,"value":"25"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":327,"order":"321.00000000000000000000","Risk-challenge":"Some AI bypasses paywalls (theyâ€™re limiting that as I type) which is unethical.","Source":[{"id":31,"value":"31"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":328,"order":"322.00000000000000000000","Risk-challenge":"Students are using generative AI to write assessments for them, in direct contradiction\nto SACE and school policies about not receiving undue assistance with assessed\nwork","Source":[{"id":31,"value":"31"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":329,"order":"323.00000000000000000000","Risk-challenge":"Challenges with understanding the underlying processes and decision-making mechanisms of AI tools","Source":[{"id":25,"value":"25"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":330,"order":"324.00000000000000000000","Risk-challenge":"Risks to trust and accountability and the need for transparency and explainability from researchers and developers","Source":[{"id":25,"value":"25"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":331,"order":"325.00000000000000000000","Risk-challenge":"Students do not always have the knowledge or critical thinking skills to identify AIgenerated falsehoods and can learn incorrect information from it.","Source":[{"id":31,"value":"31"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":332,"order":"326.00000000000000000000","Risk-challenge":"Students are using generative AI to shortcut learning activities, and in the process not\ndoing the thinking and learning the activity was designed to promote.","Source":[{"id":31,"value":"31"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":333,"order":"327.00000000000000000000","Risk-challenge":"Generative AI tools often have age-restricted terms and conditions. Schools cannot\nuse them as the sole option for activities without parental permission.","Source":[{"id":31,"value":"31"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":334,"order":"328.00000000000000000000","Risk-challenge":"Generative AI tools may be trained on 18+ or other inappropriate content that would\nnot usually be permitted in a school setting, and may generate answers based on that\ncontent","Source":[{"id":31,"value":"31"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":335,"order":"328.00000000000000000000","Risk-challenge":"Idea generation - some students use generative AI to create lists of ideas. This may\nlimit their creative skills in developing their own ideas and limit the outcome of their\nwork if the ideas from AI are overly generic and do not match what the student would\nhave been able to do without AI use.","Source":[{"id":31,"value":"31"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":336,"order":"329.00000000000000000000","Risk-challenge":"False information and hallucinations in generated content","Source":[{"id":32,"value":"32"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":337,"order":"330.00000000000000000000","Risk-challenge":"Problematic biases in outputs","Source":[{"id":32,"value":"32"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":338,"order":"331.00000000000000000000","Risk-challenge":"Data privacy and security","Source":[{"id":32,"value":"32"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":339,"order":"332.00000000000000000000","Risk-challenge":"Risks of government inaction","Source":[{"id":32,"value":"32"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":340,"order":"333.00000000000000000000","Risk-challenge":"AI literacy is a priority across the educational workforce to prepare staff and students for effective and ethical\nuse of generative AI","Source":[{"id":35,"value":"35"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":341,"order":"334.00000000000000000000","Risk-challenge":"Digital Capabilities, of which AI literacy will become one, are already uneven across providers and\nthe sector and there are disparities across the school and higher education sectors in staff and studentsâ€™ opportunities to\ndevelop digital capabilities and the new AI literacy","Source":[{"id":35,"value":"35"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":342,"order":"335.00000000000000000000","Risk-challenge":"Safe and ethical use: Bias and misinformation are widespread; intellectual property â€“ potential for copyright infringement\n(including course content) and incomplete IP claims","Source":[{"id":35,"value":"35"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":343,"order":"336.00000000000000000000","Risk-challenge":"Academic, research and graduate research integrity: Authorship; outsourcing; integrity of qualifications; assurance of\nlearning for accreditation standards","Source":[{"id":35,"value":"35"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":344,"order":"337.00000000000000000000","Risk-challenge":"Research data management and analysis: Generated â€˜dataâ€™ drawing on previous published research lacks integrity â€“cannot\nbe treated as credible representations of phenomena; generated â€˜dataâ€™ are not representative of the population being\nstudied","Source":[{"id":35,"value":"35"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":345,"order":"338.00000000000000000000","Risk-challenge":"Scholarly publishing: Insufficient detail on information sourced from generative AI for license and copyright agreements;\ngenerative AI tools do not meet authorship requirements, cannot take responsibility for a manuscript, will have difficulty\nmanaging changes in writing conventions that capitalise words in Indigenous writing","Source":[{"id":35,"value":"35"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":346,"order":"339.00000000000000000000","Risk-challenge":"Uncertainty regarding the development of AI systems and tools by innovators and\nteachers","Source":[{"id":36,"value":"36"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":347,"order":"340.00000000000000000000","Risk-challenge":"Uncertainty regarding the use of the outputs of AI by students and teachers","Source":[{"id":36,"value":"36"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":348,"order":"341.00000000000000000000","Risk-challenge":"Lack of clear permission for using copyright material in machine learning in Australian law, making the use of AI in education risky.","Source":[{"id":36,"value":"36"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":349,"order":"342.00000000000000000000","Risk-challenge":"Administrative complexities and burden of obtaining licenses for every input to AI systems, hindering practical and legal access to rich datasets for training.","Source":[{"id":36,"value":"36"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":350,"order":"343.00000000000000000000","Risk-challenge":"Absence of public interest exceptions for schools and TAFEs to engage in machine learning for educational purposes, raising doubts about the legality of teaching and using AI models in these institutions.","Source":[{"id":36,"value":"36"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":351,"order":"344.00000000000000000000","Risk-challenge":"Clear public interest exceptions needed for educational noncommercial use of generative AI to create learning resources and provide tailored learning opportunities","Source":[{"id":36,"value":"36"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":352,"order":"344.00000000000000000000","Risk-challenge":"The current problem with freely available internet materials in education is likely to worsen with the use of generative AI systems.","Source":[{"id":36,"value":"36"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":353,"order":"345.00000000000000000000","Risk-challenge":"Updating copyright laws to enable broader use of digital technologies in education, including the adoption of generative AI.","Source":[{"id":36,"value":"36"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":354,"order":"346.00000000000000000000","Risk-challenge":"Misleading, biased, incorrect or inaccurate information","Source":[{"id":37,"value":"37"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":355,"order":"347.00000000000000000000","Risk-challenge":"Academic integrity","Source":[{"id":37,"value":"37"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":356,"order":"348.00000000000000000000","Risk-challenge":"Risks and challenges around privacy and data protection","Source":[{"id":37,"value":"37"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":357,"order":"349.00000000000000000000","Risk-challenge":"The future use of the data is not known, with the potential for future systems to use input data for further upskilling","Source":[{"id":37,"value":"37"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":358,"order":"350.00000000000000000000","Risk-challenge":"Copyright and licensing\n\nGAi relies on copyright and text and data mining (TOM) provisions that are not fully incorporated into Australian copyright law at the present time","Source":[{"id":38,"value":"38"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":359,"order":"351.00000000000000000000","Risk-challenge":"Open source vs commercial Al/GAi systems\n\nThe importance of supporting open source systems which enable the code and model to be more easily interrogated, allowing for greater transparency.\nOpen Access Australasia shares concerns raised in the US and elsewhere about the risks to competition if power and usage of Al/GAi platforms and tools is concentrated among a handful!of commercial players.","Source":[{"id":38,"value":"38"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":360,"order":"352.00000000000000000000","Risk-challenge":"Lack of transparency in Al/GAi processes and systems\n\nMany widely used LLM training datasets are not fully detailed or transparent","Source":[{"id":38,"value":"38"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":361,"order":"353.00000000000000000000","Risk-challenge":"Implications for the integrity of research\n\nThere are fears that the proliferation of Al tools for academic research may result i11 decline in the quality of academic output","Source":[{"id":38,"value":"38"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":362,"order":"354.00000000000000000000","Risk-challenge":"Indigenous research and data\n\nIndigenous peoples' control of their data, culture and language is threatened when data is scraped en masse from the web without knowledge and co11sent","Source":[{"id":38,"value":"38"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":363,"order":"355.00000000000000000000","Risk-challenge":"reproduction of false, biased, or misleading information by generative AI","Source":[{"id":40,"value":"40"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":364,"order":"356.00000000000000000000","Risk-challenge":"biases exhibited by AI systems due to programming and training datasets","Source":[{"id":40,"value":"40"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":365,"order":"357.00000000000000000000","Risk-challenge":"risks associated with generative AI in learning and assessment approaches","Source":[{"id":40,"value":"40"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":366,"order":"358.00000000000000000000","Risk-challenge":"fabricated details or sources generated by AI systems","Source":[{"id":40,"value":"40"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":367,"order":"359.00000000000000000000","Risk-challenge":"legal and ethical issues regarding privacy, defamation, and intellectual property","Source":[{"id":40,"value":"40"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":368,"order":"360.00000000000000000000","Risk-challenge":"Lack of diversity in all aspects of AI, including research, development, tools design, information and content design, and learning","Source":[{"id":42,"value":"42"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":369,"order":"361.00000000000000000000","Risk-challenge":"Risk of perpetuating stereotypes, single perspective, and misinformation due to AI generating based on popular or dominant thinking","Source":[{"id":42,"value":"42"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":370,"order":"362.00000000000000000000","Risk-challenge":"High risk of misinformation, particularly regarding perspectives on gender, non-Anglo cultures, First Nations cultures, non-binary and queerness, disability, people living outside urban centers, and intersectionality within underrepresented groups","Source":[{"id":42,"value":"42"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":371,"order":"363.00000000000000000000","Risk-challenge":"Insufficient action being taken to address the lack of diversity in AI","Source":[{"id":42,"value":"42"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":372,"order":"364.00000000000000000000","Risk-challenge":"Potential for AI systems to lead to outcomes entailing prohibited discrimination","Source":[{"id":42,"value":"42"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":373,"order":"365.00000000000000000000","Risk-challenge":"AI bias having a larger effect and discriminating many people without the social control mechanisms that govern human behavior","Source":[{"id":42,"value":"42"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":374,"order":"366.00000000000000000000","Risk-challenge":"Possibility of AI systems learning biases while in operation","Source":[{"id":42,"value":"42"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":375,"order":"366.00000000000000000000","Risk-challenge":"Students may not understand how to appropriately use AI tools and be subject to\nacademic penalties","Source":[{"id":53,"value":"53"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":376,"order":"367.00000000000000000000","Risk-challenge":"Teaching staff may not know how to implement the use of AI in a safe and ethical\nway given the relative infancy of the technology in education","Source":[{"id":53,"value":"53"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":377,"order":"368.00000000000000000000","Risk-challenge":"Students may become reliant on the AI tools to the exclusion of other learning\ntechniques","Source":[{"id":53,"value":"53"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":378,"order":"369.00000000000000000000","Risk-challenge":"AI generated information may contain factual errors, bias, and may not cover all\nareas of educational content which would lead jeopardise learning outcomes if\nstudents and academics are not trained to utilize generative AI effectively Bias in AI data sets could go unchecked and become amplified","Source":[{"id":53,"value":"53"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":379,"order":"370.00000000000000000000","Risk-challenge":"The quality of education and research may suffer if bias or incorrect information is\nincorporated in data sets","Source":[{"id":53,"value":"53"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":380,"order":"371.00000000000000000000","Risk-challenge":"Misuse of AI by students has the potential to damage the academic reputation of\nuniversities","Source":[{"id":53,"value":"53"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":381,"order":"372.00000000000000000000","Risk-challenge":"AI could lead to an increase in the number of low-quality research papers in\ncirculation and jeopardise the integrity of academic publications","Source":[{"id":53,"value":"53"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":382,"order":"373.00000000000000000000","Risk-challenge":"There is a danger that low quality research could be perpetuated in future AI data\nsets","Source":[{"id":53,"value":"53"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":383,"order":"374.00000000000000000000","Risk-challenge":"Tools being used to detect the misuse of AI by students can be unreliable and might\nlead to students being falsely accused of cheating â€“ especially for the international\nstudent cohort","Source":[{"id":53,"value":"53"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":384,"order":"375.00000000000000000000","Risk-challenge":"Students personal information could be included in AI data sets which have the\npotential to be hacked or misused","Source":[{"id":53,"value":"53"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":385,"order":"376.00000000000000000000","Risk-challenge":"OECD studies (https://oecd.org/employment-outlook/2023/) and others point to a loss of 27% jobs worldwide being replaced by AI in the next few years or so, and it will only increase after that. On top of that, most of the remaining jobs will be affected/changed by AI","Source":[{"id":94,"value":"92"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":386,"order":"377.00000000000000000000","Risk-challenge":"A lot of current state of the art in AI is controlled by big tech (such as Microsoft, Amazon, Google, Apple etc), due to the huge budgets they have as well as control over many of the popular infrastructure systems we use. However, itâ€™s wise for Australia to be cautious about recommending any of these particular large commercial systems to form the basis of any of our education infrastructure in the long term. As AI systems are embedded into education, their influence over what is taught and how itâ€™s taught can grow, and quite apart from the privacy concerns which are well-known already, there is a real danger to allowing large profit-focussed global companies to have this kind of control over the curriculum and education of our future generations. Even when accuracy of information is less of a problem, there are certainly many cultural biases and curation of knowledge inherent in their design that can have possibly negative consequences for us here in Australia","Source":[{"id":94,"value":"92"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":390,"order":"378.00000000000000000000","Risk-challenge":"One of the major concerns regarding the use of generative AI in education is ensuring that students maintain the core principles of academic integrity through their work. There are legitimate concerns about the use of generative AI such as ChatGPT for plagiarism. Generative AI is not unique in its disruption of education. Shortcuts facilitated by various social and technological innovations, whilst often being beneficial, can also be a hindrance to learning. DECYP, like other jurisdictions, is concerned about the potential for new tools to be misused.","Source":[{"id":97,"value":"96"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]},{"id":391,"order":"379.00000000000000000000","Risk-challenge":"DECYP recognises that there are also challenges and risks associated with AI. Privacy and security is a significant risk, particularly when it comes to collecting and storing personal data. Also, with some AI platforms, it is recognised that users must be 18 years or older, precluding many school-aged children from using it. Bias in algorithms or incorrect information is also a concern, as the outcomes produced by generative AI are only as good as the data it has been trained on. Accountability is another factor â€“ if something goes wrong, who is held accountable? These challenges must be considered and addressed before any school can successfully implement AI in teaching and learning","Source":[{"id":97,"value":"96"}],"Tags":[],"Recommendations":[],"Recommendations - Address-challenge":[]}]
